{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83a08e1-5041-4642-99f5-c82449e37554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6526e20d-c98e-48d0-b5bb-b5f808ede823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c86942-eb85-4fa4-a306-63774d0c8264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24050\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전체 행의 수\n",
    "total_rows = len(data)\n",
    "\n",
    "# 0.1%에 해당하는 스레숄드 계산\n",
    "threshold = total_rows * 0.001\n",
    "\n",
    "# 'machinery' 칼럼에서 값별 빈도를 계산\n",
    "machinery_counts = data['Machinery'].value_counts()\n",
    "\n",
    "# 0.1% 이하로 등장하는 값 필터링\n",
    "rare_machinery = machinery_counts[machinery_counts <= threshold].index\n",
    "\n",
    "# 드롭: 'machinery' 값이 0.1% 이하인 행을 제외한 새로운 데이터프레임 생성\n",
    "filtered_data = data[~data['Machinery'].isin(rare_machinery)]\n",
    "\n",
    "# 필터링된 데이터 확인\n",
    "print(len(filtered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c7432c-d7d4-4f38-8baa-ef842df72dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('filtered_machinery.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64cb3b8e-7afc-406b-b683-e9fc57c549b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['Machinery'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f68d784-6d51-43a8-8ef6-bbad5d5109d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 괄호 안의 내용 제거\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # 특수 문자 제거 (알파벳, 숫자, 일부 허용된 특수문자 제외)\n",
    "    text = re.sub(r'[^\\w\\s\\*\\-\\+/.,]', '', text)\n",
    "    # 여러 공백을 언더스코어로 변환\n",
    "    text = re.sub(r'\\s+', '_', text)\n",
    "    # 텍스트 중간의 연속된 언더스코어를 하나로 줄임\n",
    "    text = re.sub(r'_+', '_', text)\n",
    "    # 중간에 언더스코어가 불필요하게 남아있는 경우 처리\n",
    "    text = re.sub(r'(?<!\\w)_(?!\\w)', '', text)\n",
    "    # 언더스코어 앞뒤로 존재하는 특수문자 제거\n",
    "    text = re.sub(r'_([^\\w]+)_', '_', text)\n",
    "    text = re.sub(r'_([^\\w]+)$', '', text)\n",
    "    text = re.sub(r'^([^\\w]+)_', '', text)\n",
    "    # 텍스트 끝부분의 불필요한 언더스코어 제거\n",
    "    text = re.sub(r'_+$', '', text)\n",
    "    # 영어 단어는 소문자로 변환\n",
    "    text = ' '.join([word.lower() if re.match(r'[A-Za-z]', word) else word for word in text.split()])\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_supplier_name(name):\n",
    "    # 접미사 제거\n",
    "    suffixes = r'\\b(Corp\\.?|Corporation|Company|Co\\.?|Incorporated|Inc\\.?|Limited|Ltd\\.?|GmbH|S\\.L\\.|SDN\\. BHD\\.)\\b'\n",
    "    name = re.sub(suffixes, '', name, flags=re.IGNORECASE)\n",
    "    # 특수 문자 제거\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    # 불필요한 단어 제거\n",
    "    name = re.sub(r'\\b(사용금지|사)\\b', '', name, flags=re.IGNORECASE)\n",
    "    # 공백 정리\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    # 오타 수정 및 문자열 정리\n",
    "    name = re.sub(r'coporation|coropration|coproration|corporration', 'corporation', name, flags=re.IGNORECASE)\n",
    "    name = name.lower().strip()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e38477e-fa6b-4076-a3da-5af07d2a02cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 칼럼 전처리\n",
    "data['cleaned_item'] = data['청구품목'].apply(preprocess_text)\n",
    "data['cleaned_supplier'] = data['발주처'].apply(clean_supplier_name)\n",
    "\n",
    "# 전처리된 칼럼 결합\n",
    "data['combined_text'] =data['cleaned_item'].fillna('') + \" \" + data['cleaned_supplier'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c685cdd-83a5-452c-b78e-bcd3c53af7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     combined_text\n",
      "0      ge_power_pack_fork_e7 matsuiusa corporation\n",
      "1      ge_power_pack_fork_e7 matsuiusa corporation\n",
      "2                  h-ex_30_4_1/4,_100md_120fms kti\n",
      "3                  nylon_54_4_1/4,_100md_50fms kti\n",
      "4                  nylon_48_4_1/4,_100md_50fms kti\n",
      "...                                            ...\n",
      "24045             ring-o haein corporation_cheonan\n",
      "24046     ring-retaining haein corporation_cheonan\n",
      "24047     sleeve-bearing haein corporation_cheonan\n",
      "24048       bearing-ball haein corporation_cheonan\n",
      "24049    bearing-ball_de haein corporation_cheonan\n",
      "\n",
      "[24050 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data[['combined_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00759605-3dff-49b1-a721-3bfb6e93fb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353290, 712710)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText, Word2Vec\n",
    "import torch\n",
    "\n",
    "# 문장을 토큰화하여 리스트로 만들어야 합니다.\n",
    "sentences = [text.split() for text in data['combined_text']]\n",
    "\n",
    "# FastText 모델 학습\n",
    "ft_model = FastText(vector_size=100, window=5, min_count=1, min_n=3, max_n=6, sg=1)\n",
    "ft_model.build_vocab(sentences)\n",
    "ft_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "w2v_model.train(sentences, total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02e2a13-a066-43a4-9dfd-5b14d1bfc8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a503488c-0234-4421-897f-2bbf50e572d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24050, 200])\n"
     ]
    }
   ],
   "source": [
    "# FastText 임베딩을 가져오는 함수\n",
    "def get_embedding(word, model):\n",
    "    if word in model.wv:\n",
    "        return torch.tensor(model.wv[word])\n",
    "    else:\n",
    "        # 서브워드 임베딩의 평균을 계산\n",
    "        subwords = [word[i:j] for i in range(len(word)) for j in range(i+1, len(word)+1)]\n",
    "        subword_vectors = [model.wv[subword] for subword in subwords if subword in model.wv]\n",
    "        \n",
    "        if subword_vectors:\n",
    "            return torch.tensor(subword_vectors).mean(dim=0)\n",
    "        else:\n",
    "            return torch.zeros(model.vector_size)  # 단어가 없는 경우 0 벡터로 처리\n",
    "# FastText 임베딩과 Word2Vec 임베딩을 결합한 함수\n",
    "def get_combined_embedding(word, ft_model, w2v_model):\n",
    "    ft_vector = get_embedding(word, ft_model)  # FastText에서 얻은 임베딩\n",
    "    if word in w2v_model.wv:\n",
    "        w2v_vector = torch.tensor(w2v_model.wv[word])  # Word2Vec에서 얻은 임베딩\n",
    "    else:\n",
    "        w2v_vector = torch.zeros(w2v_model.vector_size)  # 단어가 없는 경우 0 벡터로 처리\n",
    "\n",
    "    combined_vector = torch.cat((ft_vector, w2v_vector))  # 두 임베딩을 결합 (concatenate)\n",
    "    return combined_vector\n",
    "\n",
    "# 결합된 임베딩을 생성\n",
    "combined_embeddings = []\n",
    "for text in data['combined_text']:\n",
    "    words = text.split()\n",
    "    word_vectors = [get_combined_embedding(word, ft_model, w2v_model) for word in words]\n",
    "    if word_vectors:\n",
    "        embedding = torch.stack(word_vectors).mean(dim=0)  # 단어 벡터의 평균 계산\n",
    "    else:\n",
    "        embedding = torch.zeros(model.vector_size + w2v_model.vector_size)  # 단어가 없는 경우 0 벡터로 처리\n",
    "    combined_embeddings.append(embedding)\n",
    "\n",
    "# 결합된 임베딩 리스트를 텐서로 변환\n",
    "combined_embeddings_tensor = torch.stack(combined_embeddings)\n",
    "\n",
    "print(combined_embeddings_tensor.shape)  # 결합된 임베딩 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5b25e1-80b7-4ac9-82de-58a59d5b6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 모든 단어에 대해 결합된 임베딩을 계산하고 저장\n",
    "combined_word_vectors = {}\n",
    "for word in ft_model.wv.index_to_key:  # FastText 모델의 모든 단어에 대해 반복\n",
    "    combined_word_vectors[word] = get_combined_embedding(word, ft_model, w2v_model)\n",
    "\n",
    "# 특정 단어와 가장 유사한 단어 5개를 찾는 함수 정의\n",
    "def find_similar_words(target_word, combined_word_vectors, topn=5):\n",
    "    if target_word not in combined_word_vectors:\n",
    "        print(f\"Word '{target_word}' not in vocabulary.\")\n",
    "        return []\n",
    "\n",
    "    target_vector = combined_word_vectors[target_word]\n",
    "    similarities = {}\n",
    "\n",
    "    for word, vector in combined_word_vectors.items():\n",
    "        similarity = F.cosine_similarity(target_vector.unsqueeze(0), vector.unsqueeze(0)).item()\n",
    "        similarities[word] = similarity\n",
    "\n",
    "    # 상위 topn개의 유사 단어를 찾음\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_similarities[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e2a223d-696f-461e-a040-02de1a1fbeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'valve':\n",
      "valve: 1.0000\n",
      "check_valve: 0.9829\n",
      "cover_gasket: 0.9822\n",
      "connector: 0.9821\n",
      "valve_sun_cbcg-ljn: 0.9804\n"
     ]
    }
   ],
   "source": [
    "# 특정 단어의 유사 단어 찾기\n",
    "word = \"valve\".lower()   # 여기에는 확인하고 싶은 단어를 넣으세요\n",
    "similar_words = find_similar_words(word, combined_word_vectors, topn=5)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Words most similar to '{word}':\")\n",
    "for similar_word, similarity in similar_words:\n",
    "    print(f\"{similar_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf009e4d-8dd6-4e58-a3ad-df4efd1bdd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. 데이터 준비 및 인코딩\n",
    "machinery = data['Machinery'].values\n",
    "machinery_encoder = LabelEncoder()\n",
    "machinery_labels = machinery_encoder.fit_transform(machinery)\n",
    "\n",
    "# 결합된 임베딩 텐서를 넘파이 배열로 변환\n",
    "X = combined_embeddings_tensor.numpy()\n",
    "\n",
    "# 2. Train-Test Split (각 레이블에 대해 동일한 분할 사용)\n",
    "X_train_val, X_test, y_train_val_machinery, y_test_machinery = train_test_split(\n",
    "    X, machinery_labels, test_size=0.2, random_state=42, stratify=machinery_labels\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train_machinery, y_val_machinery = train_test_split(\n",
    "    X_train_val, y_train_val_machinery, test_size=0.2, random_state=42, stratify=y_train_val_machinery\n",
    ")\n",
    "\n",
    "\n",
    "# 4. 데이터 정규화 (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_val_normalized = scaler.transform(X_val)  # 검증 데이터 정규화\n",
    "X_test_normalized = scaler.transform(X_test)  # 테스트 데이터 정규화\n",
    "\n",
    "\n",
    "# 5. Train 데이터를 torch Tensor로 변환\n",
    "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_normalized, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32).to(device)\n",
    "\n",
    "y_train_machinery_tensor = torch.tensor(y_train_machinery, dtype=torch.long).to(device)\n",
    "y_val_machinery_tensor = torch.tensor(y_val_machinery, dtype=torch.long).to(device)\n",
    "y_test_machinery_tensor = torch.tensor(y_test_machinery, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b3e7fe7-8633-4dda-a870-1e1bf389c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Transformer 기반 모델 정의\n",
    "class MachineryTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, output_dim, dropout=0.3):\n",
    "        super(MachineryTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)  # 입력 차원을 Transformer 입력 차원(hidden_dim)으로 변환\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 최종 분류를 위한 출력 레이어\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # 입력을 hidden_dim 크기로 변환\n",
    "        x = x.unsqueeze(1)  # 시퀀스 차원 추가 -> (batch_size, sequence_length=1, hidden_dim)\n",
    "        x = self.transformer_encoder(x)  # Transformer 인코더 레이어\n",
    "        x = x.mean(dim=1)  # 각 시퀀스의 평균을 사용해 문장 임베딩을 생성\n",
    "        x = self.fc(x)  # 분류를 위한 출력 레이어\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2a2dedc-3a5f-4ab8-a581-e2e726f9e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "input_dim = X_train_tensor.shape[1]  # 임베딩 차원 (200)\n",
    "hidden_dim = 256  # Transformer 은닉 상태 크기\n",
    "num_heads = 4  # Multi-head attention의 head 수\n",
    "num_layers = 2  # Transformer 레이어 수\n",
    "output_dim = len(machinery_encoder.classes_)  # 분류할 클래스의 수\n",
    "\n",
    "model = MachineryTransformer(input_dim, hidden_dim, num_heads, num_layers, output_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "585f68be-cc29-4cad-8eee-a5ceba86bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MachineryTransformer(\n",
      "  (embedding): Linear(in_features=200, out_features=128, bias=True)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.3, inplace=False)\n",
      "        (dropout2): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=76, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce13273b-4cf6-4b78-b24f-45068cec6374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()  # 분류 문제이므로 CrossEntropyLoss 사용\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 옵티마이저 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1c9df29-29c1-4043-a8b6-aad4b676bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 1. TensorDataset과 DataLoader로 데이터셋을 배치 단위로 묶음\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_machinery_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_machinery_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_machinery_tensor)\n",
    "\n",
    "# DataLoader로 배치 단위로 데이터를 나누기\n",
    "batch_size = 64  # 원하는 배치 크기\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8fdd325-9b2f-4d90-85d9-c38ecd101049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프 정의\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # 검증 데이터에서 성능 확인\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                val_outputs = model(X_batch)\n",
    "                val_loss += criterion(val_outputs, y_batch).item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "207b1712-272f-45ab-9da1-b0d7407e9cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 457.3410, Val Loss: 101.7729\n",
      "Epoch [2/10], Loss: 399.5066, Val Loss: 94.1813\n",
      "Epoch [3/10], Loss: 382.7635, Val Loss: 91.9652\n",
      "Epoch [4/10], Loss: 368.9719, Val Loss: 90.3656\n",
      "Epoch [5/10], Loss: 363.8580, Val Loss: 88.8540\n",
      "Epoch [6/10], Loss: 358.8641, Val Loss: 89.5257\n",
      "Epoch [7/10], Loss: 353.6300, Val Loss: 88.4274\n",
      "Epoch [8/10], Loss: 349.4143, Val Loss: 85.3485\n",
      "Epoch [9/10], Loss: 346.3661, Val Loss: 85.0262\n",
      "Epoch [10/10], Loss: 343.2732, Val Loss: 86.0768\n"
     ]
    }
   ],
   "source": [
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c46f3a-a38a-4168-923a-acffab077fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ship)",
   "language": "python",
   "name": "ship"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
