{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83a08e1-5041-4642-99f5-c82449e37554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6526e20d-c98e-48d0-b5bb-b5f808ede823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d1ff7f-9042-4774-94de-dd80bd002a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('filtered_dataset0.1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bebfe21e-476b-47be-8898-5292f62584e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15162 entries, 0 to 15161\n",
      "Data columns (total 32 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   청구서번호        15162 non-null  object \n",
      " 1   No.          15162 non-null  int64  \n",
      " 2   Subject      15152 non-null  object \n",
      " 3   Machinery    15162 non-null  object \n",
      " 4   Assembly     15162 non-null  object \n",
      " 5   청구품목         15162 non-null  object \n",
      " 6   Unnamed: 6   0 non-null      float64\n",
      " 7   Part No.1    15154 non-null  object \n",
      " 8   Part No.2    2614 non-null   object \n",
      " 9   청구량          15092 non-null  float64\n",
      " 10  견적           14957 non-null  object \n",
      " 11  견적수량         15092 non-null  float64\n",
      " 12  견적화폐         15092 non-null  object \n",
      " 13  견적단가         15162 non-null  float64\n",
      " 14  발주번호         15162 non-null  object \n",
      " 15  발주처          15162 non-null  object \n",
      " 16  발주           15162 non-null  object \n",
      " 17  발주수량         15092 non-null  float64\n",
      " 18  발주금액         15092 non-null  float64\n",
      " 19  D/T          13597 non-null  object \n",
      " 20  미입고 기간       1058 non-null   object \n",
      " 21  창고입고         13012 non-null  object \n",
      " 22  창고입고수량       15162 non-null  int64  \n",
      " 23  Control No.  9490 non-null   object \n",
      " 24  입고창고         13012 non-null  object \n",
      " 25  창고출고         11609 non-null  object \n",
      " 26  창고출고수량       15162 non-null  int64  \n",
      " 27  출고선박         11609 non-null  object \n",
      " 28  출고운반선        11609 non-null  object \n",
      " 29  선박입고         3307 non-null   object \n",
      " 30  선박입고수량       15162 non-null  int64  \n",
      " 31  완료 여부        3301 non-null   object \n",
      "dtypes: float64(6), int64(4), object(22)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1f68d784-6d51-43a8-8ef6-bbad5d5109d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 괄호 안의 내용 제거\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # 특수 문자 제거 (알파벳, 숫자, 일부 허용된 특수문자 제외)\n",
    "    text = re.sub(r'[^\\w\\s\\*\\-\\+/.,]', '', text)\n",
    "    # 여러 공백을 언더스코어로 변환\n",
    "    text = re.sub(r'\\s+', '_', text)\n",
    "    # 텍스트 중간의 연속된 언더스코어를 하나로 줄임\n",
    "    text = re.sub(r'_+', '_', text)\n",
    "    # 중간에 언더스코어가 불필요하게 남아있는 경우 처리\n",
    "    text = re.sub(r'(?<!\\w)_(?!\\w)', '', text)\n",
    "    # 언더스코어 앞뒤로 존재하는 특수문자 제거\n",
    "    text = re.sub(r'_([^\\w]+)_', '_', text)\n",
    "    text = re.sub(r'_([^\\w]+)$', '', text)\n",
    "    text = re.sub(r'^([^\\w]+)_', '', text)\n",
    "    # 텍스트 끝부분의 불필요한 언더스코어 제거\n",
    "    text = re.sub(r'_+$', '', text)\n",
    "    # 영어 단어는 소문자로 변환\n",
    "    text = ' '.join([word.lower() if re.match(r'[A-Za-z]', word) else word for word in text.split()])\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_supplier_name(name):\n",
    "    # 접미사 제거\n",
    "    suffixes = r'\\b(Corp\\.?|Corporation|Company|Co\\.?|Incorporated|Inc\\.?|Limited|Ltd\\.?|GmbH|S\\.L\\.|SDN\\. BHD\\.)\\b'\n",
    "    name = re.sub(suffixes, '', name, flags=re.IGNORECASE)\n",
    "    # 특수 문자 제거\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    # 불필요한 단어 제거\n",
    "    name = re.sub(r'\\b(사용금지|사)\\b', '', name, flags=re.IGNORECASE)\n",
    "    # 공백 정리\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    # 오타 수정 및 문자열 정리\n",
    "    name = re.sub(r'coporation|coropration|coproration|corporration', 'corporation', name, flags=re.IGNORECASE)\n",
    "    name = name.lower().strip()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0e38477e-fa6b-4076-a3da-5af07d2a02cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 칼럼 전처리\n",
    "data['cleaned_item'] = data['청구품목'].apply(preprocess_text)\n",
    "data['cleaned_supplier'] = data['발주처'].apply(clean_supplier_name)\n",
    "\n",
    "# 전처리된 칼럼 결합\n",
    "data['combined_text'] =data['cleaned_item'].fillna('') + \" \" + data['cleaned_supplier'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2c685cdd-83a5-452c-b78e-bcd3c53af7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     combined_text\n",
      "0      ge_power_pack_fork_e7 matsuiusa corporation\n",
      "1      ge_power_pack_fork_e7 matsuiusa corporation\n",
      "2                  nylon_54_4_1/4,_100md_50fms kti\n",
      "3                  nylon_48_4_1/4,_100md_50fms kti\n",
      "4                  nylon_42_4_1/4,_100md_50fms kti\n",
      "...                                            ...\n",
      "15157             ring-o haein corporation_cheonan\n",
      "15158     ring-retaining haein corporation_cheonan\n",
      "15159     sleeve-bearing haein corporation_cheonan\n",
      "15160       bearing-ball haein corporation_cheonan\n",
      "15161    bearing-ball_de haein corporation_cheonan\n",
      "\n",
      "[15162 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data[['combined_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "463a83f9-bb98-4700-b241-b8c006c59b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216309, 443560)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText, Word2Vec\n",
    "import torch\n",
    "\n",
    "# 문장을 토큰화하여 리스트로 만들어야 합니다.\n",
    "sentences = [text.split() for text in data['combined_text']]\n",
    "\n",
    "# FastText 모델 학습\n",
    "ft_model = FastText(vector_size=100, window=5, min_count=1, min_n=3, max_n=6, sg=1)\n",
    "ft_model.build_vocab(sentences)\n",
    "ft_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "w2v_model.train(sentences, total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "166e9254-3a18-40f4-a5fd-20f0796fdcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15162, 200])\n"
     ]
    }
   ],
   "source": [
    "# FastText 임베딩을 가져오는 함수\n",
    "def get_embedding(word, model):\n",
    "    if word in model.wv:\n",
    "        return torch.tensor(model.wv[word])\n",
    "    else:\n",
    "        # 서브워드 임베딩의 평균을 계산\n",
    "        subwords = [word[i:j] for i in range(len(word)) for j in range(i+1, len(word)+1)]\n",
    "        subword_vectors = [model.wv[subword] for subword in subwords if subword in model.wv]\n",
    "        \n",
    "        if subword_vectors:\n",
    "            return torch.tensor(subword_vectors).mean(dim=0)\n",
    "        else:\n",
    "            return torch.zeros(model.vector_size)  # 단어가 없는 경우 0 벡터로 처리\n",
    "# FastText 임베딩과 Word2Vec 임베딩을 결합한 함수\n",
    "def get_combined_embedding(word, ft_model, w2v_model):\n",
    "    ft_vector = get_embedding(word, ft_model)  # FastText에서 얻은 임베딩\n",
    "    if word in w2v_model.wv:\n",
    "        w2v_vector = torch.tensor(w2v_model.wv[word])  # Word2Vec에서 얻은 임베딩\n",
    "    else:\n",
    "        w2v_vector = torch.zeros(w2v_model.vector_size)  # 단어가 없는 경우 0 벡터로 처리\n",
    "\n",
    "    combined_vector = torch.cat((ft_vector, w2v_vector))  # 두 임베딩을 결합 (concatenate)\n",
    "    return combined_vector\n",
    "\n",
    "# 결합된 임베딩을 생성\n",
    "combined_embeddings = []\n",
    "for text in data['combined_text']:\n",
    "    words = text.split()\n",
    "    word_vectors = [get_combined_embedding(word, ft_model, w2v_model) for word in words]\n",
    "    if word_vectors:\n",
    "        embedding = torch.stack(word_vectors).mean(dim=0)  # 단어 벡터의 평균 계산\n",
    "    else:\n",
    "        embedding = torch.zeros(model.vector_size + w2v_model.vector_size)  # 단어가 없는 경우 0 벡터로 처리\n",
    "    combined_embeddings.append(embedding)\n",
    "\n",
    "# 결합된 임베딩 리스트를 텐서로 변환\n",
    "combined_embeddings_tensor = torch.stack(combined_embeddings)\n",
    "\n",
    "print(combined_embeddings_tensor.shape)  # 결합된 임베딩 텐서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "208ec7e7-b472-4213-b596-9e04300142c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 모든 단어에 대해 결합된 임베딩을 계산하고 저장\n",
    "combined_word_vectors = {}\n",
    "for word in ft_model.wv.index_to_key:  # FastText 모델의 모든 단어에 대해 반복\n",
    "    combined_word_vectors[word] = get_combined_embedding(word, ft_model, w2v_model)\n",
    "\n",
    "# 특정 단어와 가장 유사한 단어 5개를 찾는 함수 정의\n",
    "def find_similar_words(target_word, combined_word_vectors, topn=5):\n",
    "    if target_word not in combined_word_vectors:\n",
    "        print(f\"Word '{target_word}' not in vocabulary.\")\n",
    "        return []\n",
    "\n",
    "    target_vector = combined_word_vectors[target_word]\n",
    "    similarities = {}\n",
    "\n",
    "    for word, vector in combined_word_vectors.items():\n",
    "        similarity = F.cosine_similarity(target_vector.unsqueeze(0), vector.unsqueeze(0)).item()\n",
    "        similarities[word] = similarity\n",
    "\n",
    "    # 상위 topn개의 유사 단어를 찾음\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_similarities[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "83341603-c6d4-4c69-9b90-9ad4bccefdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector for 'screw': tensor([-2.6953e-01,  1.0618e+00,  9.7410e-02,  8.4718e-02, -1.4775e-01,\n",
      "         3.5462e-01,  2.9825e-01,  2.1511e-01, -6.5101e-01,  2.9122e-01,\n",
      "        -4.4524e-01,  4.6695e-01, -6.2215e-02,  3.8362e-01,  3.8894e-01,\n",
      "         9.5985e-02, -9.2560e-02, -4.6851e-01,  2.4317e-01, -4.0242e-01,\n",
      "        -1.1162e-01, -8.0334e-01, -2.9055e-01,  3.7296e-01,  5.1049e-01,\n",
      "        -4.0715e-01,  2.8165e-01, -7.3628e-01,  4.3052e-01,  1.3294e-01,\n",
      "         5.5091e-01,  1.5341e-02, -3.1327e-03, -1.1906e-01,  3.4249e-01,\n",
      "         6.6751e-01,  3.4811e-01,  3.3393e-01, -9.9389e-01,  2.7712e-01,\n",
      "         2.8915e-01, -1.1394e+00,  1.0444e-01, -9.9757e-02, -3.5578e-01,\n",
      "        -7.7339e-01,  2.0051e-01, -1.0524e-01, -1.9788e-01, -2.9482e-02,\n",
      "         6.0938e-01,  4.6622e-01,  5.4943e-02,  7.0522e-01,  2.0831e-01,\n",
      "        -3.7474e-02, -1.8525e-01, -5.6877e-01, -2.3078e-01, -6.2758e-01,\n",
      "        -2.9584e-01, -3.1617e-01, -8.6180e-01,  3.4211e-01,  8.8007e-02,\n",
      "        -2.9705e-01, -9.2229e-03, -2.8504e-01, -7.0909e-02,  1.7157e-01,\n",
      "        -7.2803e-02, -3.6506e-01, -4.0334e-01, -1.5535e-01,  1.1614e-01,\n",
      "         3.7193e-01,  5.2083e-01, -2.6849e-01,  3.5138e-01,  1.2638e-01,\n",
      "         4.5039e-01,  1.4441e-01,  2.5618e-01, -6.4073e-01,  3.8827e-01,\n",
      "         3.0004e-01, -4.9406e-02, -2.4953e-01, -4.0125e-01,  6.7969e-02,\n",
      "        -8.2362e-02, -2.8304e-01,  1.7047e-01, -6.1394e-01,  6.5408e-01,\n",
      "        -1.8833e-01,  8.2472e-02,  6.3777e-02,  3.6402e-01, -1.4313e-02,\n",
      "         4.3118e-02,  8.8424e-02,  1.7079e-02, -1.2992e-01,  9.2549e-02,\n",
      "        -3.1526e-01, -9.6972e-02,  6.8546e-01, -1.5524e-01, -3.7788e-01,\n",
      "        -3.6519e-01, -3.4711e-01,  5.0081e-02,  2.7356e-01,  7.8627e-04,\n",
      "        -3.6862e-01,  8.2103e-02,  9.4292e-03,  1.6692e-01,  1.7240e-02,\n",
      "         1.4685e-01, -2.8691e-01,  1.6494e-02,  1.1579e-01,  5.4574e-02,\n",
      "        -1.3143e-01,  1.4799e-01,  1.2955e-02,  9.4180e-02,  8.1043e-02,\n",
      "         1.1344e-01, -4.0165e-01, -4.1795e-01, -8.2529e-01,  3.2623e-02,\n",
      "         3.4447e-01,  3.7263e-01, -1.3523e-01, -1.4718e-01,  1.1159e-01,\n",
      "         2.6224e-01, -3.7112e-01, -2.4305e-02,  9.9925e-02,  2.9395e-01,\n",
      "        -3.2138e-02, -2.7676e-01, -3.1964e-01, -8.5966e-02, -3.4209e-01,\n",
      "         3.9834e-01,  4.8978e-02, -1.3363e-01, -1.5576e-01,  6.8663e-02,\n",
      "        -1.4818e-02, -1.1481e-01, -2.4251e-01, -8.7067e-02, -3.0726e-01,\n",
      "        -1.6821e-01, -2.7452e-01,  3.8501e-01,  4.8570e-01,  1.0608e-04,\n",
      "         5.2667e-01,  3.6080e-02,  7.4507e-02, -3.0875e-01,  2.3173e-01,\n",
      "         1.3000e-02, -8.8296e-02,  2.0618e-01, -9.1988e-02,  1.0837e-01,\n",
      "         3.4448e-01, -2.4298e-01,  6.4721e-02, -1.4504e-02,  6.0813e-01,\n",
      "        -2.1479e-01, -5.3247e-02,  7.0772e-04, -6.6302e-02, -1.3371e-01,\n",
      "        -5.7772e-02,  2.0549e-01,  2.9316e-01,  4.6812e-01,  3.3355e-01,\n",
      "         1.4250e-01, -8.2610e-02,  2.7047e-01, -1.3116e-01,  2.7155e-01,\n",
      "        -2.4452e-02, -1.9565e-01,  2.6646e-02,  8.5376e-02, -7.9752e-02])\n"
     ]
    }
   ],
   "source": [
    "word = \"SCREW\".lower()  # 소문자로 변환\n",
    "if word in combined_word_vectors:\n",
    "    print(f\"Embedding vector for '{word}': {combined_word_vectors[word]}\")\n",
    "else:\n",
    "    print(f\"Word '{word}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5f396888-1d67-4912-89ca-5f40b388ec93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'valve':\n",
      "valve: 1.0000\n",
      "check_valve: 0.9892\n",
      "valve_c/bal_h-1180: 0.9868\n",
      "valve_pcl402-db_sae: 0.9867\n",
      "bearing_cover: 0.9846\n"
     ]
    }
   ],
   "source": [
    "# 특정 단어의 유사 단어 찾기\n",
    "word = \"valve\".lower()   # 여기에는 확인하고 싶은 단어를 넣으세요\n",
    "similar_words = find_similar_words(word, combined_word_vectors, topn=5)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Words most similar to '{word}':\")\n",
    "for similar_word, similarity in similar_words:\n",
    "    print(f\"{similar_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fe227dba-0285-4352-ad25-c1da4b7e51e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1b404d9b-ac94-4ef0-9d92-dcfd47ce6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. 데이터 준비 및 인코딩\n",
    "machinery = data['Machinery'].values\n",
    "assembly = data['Assembly'].values\n",
    "\n",
    "machinery_encoder = LabelEncoder()\n",
    "assembly_encoder = LabelEncoder()\n",
    "\n",
    "machinery_labels = machinery_encoder.fit_transform(machinery)\n",
    "assembly_labels = assembly_encoder.fit_transform(assembly)\n",
    "\n",
    "# 임베딩을 numpy 배열로 변환\n",
    "X = combined_embeddings_tensor.numpy()\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  \n",
    "\n",
    "# PCA를 통한 차원 축소\n",
    "pca = PCA(n_components=100)  # 예시로 50개의 주성분만 유지\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 데이터셋을 훈련, 검증, 테스트 세트로 분할\n",
    "X_train_val, X_test, y_train_val_machinery, y_test_machinery, y_train_val_assembly, y_test_assembly = train_test_split(\n",
    "    X_pca,  # PCA 적용 데이터 사용\n",
    "    machinery_labels,  # 머시너리 레이블\n",
    "    assembly_labels,  # 어셈블리 레이블\n",
    "    test_size=0.2,  # 테스트 세트는 전체 데이터의 20%\n",
    "    random_state=42  # 난수 생성기 시드 값\n",
    ")\n",
    "\n",
    "# 훈련 세트를 다시 훈련과 검증 세트로 분할\n",
    "X_train, X_val, y_train_machinery, y_val_machinery, y_train_assembly, y_val_assembly = train_test_split(\n",
    "    X_train_val,  # 훈련 및 검증 세트\n",
    "    y_train_val_machinery,  # 훈련 및 검증 머시너리 레이블\n",
    "    y_train_val_assembly,  # 훈련 및 검증 어셈블리 레이블\n",
    "    test_size=0.25,  # 검증 세트는 훈련 및 검증 세트의 25% (전체 데이터의 20%)\n",
    "    random_state=42  # 난수 생성기 시드 값\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d5f2a704-bc3f-4b82-baf1-d5f9b9f18d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install conda-forge::catboost -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f99881a9-1898-46fd-9725-68408fd2790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5198989\ttest: 0.5225849\tbest: 0.5225849 (0)\ttotal: 888ms\tremaining: 7m 22s\n",
      "10:\tlearn: 0.5731091\ttest: 0.5700626\tbest: 0.5700626 (10)\ttotal: 8.68s\tremaining: 6m 25s\n",
      "20:\tlearn: 0.5914688\ttest: 0.5865480\tbest: 0.5865480 (20)\ttotal: 16.4s\tremaining: 6m 14s\n",
      "30:\tlearn: 0.6035620\ttest: 0.5970986\tbest: 0.5970986 (30)\ttotal: 24.2s\tremaining: 6m 6s\n",
      "40:\tlearn: 0.6142260\ttest: 0.6073195\tbest: 0.6073195 (40)\ttotal: 32s\tremaining: 5m 58s\n",
      "50:\tlearn: 0.6225814\ttest: 0.6142433\tbest: 0.6142433 (50)\ttotal: 39.8s\tremaining: 5m 50s\n",
      "60:\tlearn: 0.6299472\ttest: 0.6195186\tbest: 0.6195186 (60)\ttotal: 47.6s\tremaining: 5m 42s\n",
      "70:\tlearn: 0.6409411\ttest: 0.6294098\tbest: 0.6300692 (67)\ttotal: 55.3s\tremaining: 5m 34s\n",
      "80:\tlearn: 0.6519349\ttest: 0.6369931\tbest: 0.6369931 (80)\ttotal: 1m 3s\tremaining: 5m 26s\n",
      "90:\tlearn: 0.6608399\ttest: 0.6452357\tbest: 0.6452357 (89)\ttotal: 1m 10s\tremaining: 5m 18s\n",
      "100:\tlearn: 0.6691953\ttest: 0.6524893\tbest: 0.6524893 (100)\ttotal: 1m 18s\tremaining: 5m 10s\n",
      "110:\tlearn: 0.6797493\ttest: 0.6610617\tbest: 0.6610617 (110)\ttotal: 1m 26s\tremaining: 5m 2s\n",
      "120:\tlearn: 0.6846966\ttest: 0.6636993\tbest: 0.6636993 (120)\ttotal: 1m 34s\tremaining: 4m 55s\n",
      "130:\tlearn: 0.6954705\ttest: 0.6663370\tbest: 0.6663370 (130)\ttotal: 1m 42s\tremaining: 4m 47s\n",
      "140:\tlearn: 0.7017370\ttest: 0.6712826\tbest: 0.6712826 (140)\ttotal: 1m 49s\tremaining: 4m 39s\n",
      "150:\tlearn: 0.7070141\ttest: 0.6732608\tbest: 0.6739202 (147)\ttotal: 1m 57s\tremaining: 4m 31s\n",
      "160:\tlearn: 0.7150396\ttest: 0.6785361\tbest: 0.6785361 (160)\ttotal: 2m 5s\tremaining: 4m 24s\n",
      "170:\tlearn: 0.7194371\ttest: 0.6801846\tbest: 0.6801846 (170)\ttotal: 2m 13s\tremaining: 4m 16s\n",
      "180:\tlearn: 0.7258135\ttest: 0.6831520\tbest: 0.6834817 (175)\ttotal: 2m 20s\tremaining: 4m 7s\n",
      "190:\tlearn: 0.7333993\ttest: 0.6871085\tbest: 0.6871085 (190)\ttotal: 2m 28s\tremaining: 3m 59s\n",
      "200:\tlearn: 0.7355981\ttest: 0.6890867\tbest: 0.6897461 (196)\ttotal: 2m 35s\tremaining: 3m 51s\n",
      "210:\tlearn: 0.7392260\ttest: 0.6920541\tbest: 0.6920541 (205)\ttotal: 2m 43s\tremaining: 3m 44s\n",
      "220:\tlearn: 0.7439534\ttest: 0.6930432\tbest: 0.6930432 (220)\ttotal: 2m 51s\tremaining: 3m 36s\n",
      "230:\tlearn: 0.7481310\ttest: 0.6950214\tbest: 0.6953511 (229)\ttotal: 2m 58s\tremaining: 3m 28s\n",
      "240:\tlearn: 0.7543975\ttest: 0.6969997\tbest: 0.6976591 (237)\ttotal: 3m 6s\tremaining: 3m 20s\n",
      "250:\tlearn: 0.7569261\ttest: 0.6996373\tbest: 0.6996373 (250)\ttotal: 3m 14s\tremaining: 3m 13s\n",
      "260:\tlearn: 0.7601143\ttest: 0.6999670\tbest: 0.7006264 (257)\ttotal: 3m 22s\tremaining: 3m 5s\n",
      "270:\tlearn: 0.7628628\ttest: 0.6996373\tbest: 0.7012859 (261)\ttotal: 3m 30s\tremaining: 2m 57s\n",
      "280:\tlearn: 0.7652814\ttest: 0.7016156\tbest: 0.7016156 (278)\ttotal: 3m 37s\tremaining: 2m 49s\n",
      "290:\tlearn: 0.7680299\ttest: 0.7035938\tbest: 0.7035938 (289)\ttotal: 3m 45s\tremaining: 2m 41s\n",
      "300:\tlearn: 0.7703386\ttest: 0.7049126\tbest: 0.7049126 (300)\ttotal: 3m 52s\tremaining: 2m 33s\n",
      "310:\tlearn: 0.7739666\ttest: 0.7059017\tbest: 0.7059017 (308)\ttotal: 4m\tremaining: 2m 26s\n",
      "320:\tlearn: 0.7763852\ttest: 0.7082097\tbest: 0.7082097 (320)\ttotal: 4m 8s\tremaining: 2m 18s\n",
      "330:\tlearn: 0.7786939\ttest: 0.7082097\tbest: 0.7082097 (320)\ttotal: 4m 15s\tremaining: 2m 10s\n",
      "340:\tlearn: 0.7801231\ttest: 0.7091988\tbest: 0.7091988 (338)\ttotal: 4m 23s\tremaining: 2m 2s\n",
      "350:\tlearn: 0.7828716\ttest: 0.7108473\tbest: 0.7115068 (349)\ttotal: 4m 30s\tremaining: 1m 54s\n",
      "360:\tlearn: 0.7850704\ttest: 0.7115068\tbest: 0.7115068 (349)\ttotal: 4m 38s\tremaining: 1m 47s\n",
      "370:\tlearn: 0.7875989\ttest: 0.7118365\tbest: 0.7121662 (368)\ttotal: 4m 45s\tremaining: 1m 39s\n",
      "380:\tlearn: 0.7890281\ttest: 0.7128256\tbest: 0.7128256 (379)\ttotal: 4m 53s\tremaining: 1m 31s\n",
      "390:\tlearn: 0.7904573\ttest: 0.7134850\tbest: 0.7134850 (383)\ttotal: 5m 1s\tremaining: 1m 23s\n",
      "400:\tlearn: 0.7914468\ttest: 0.7131553\tbest: 0.7141444 (399)\ttotal: 5m 8s\tremaining: 1m 16s\n",
      "410:\tlearn: 0.7944151\ttest: 0.7148038\tbest: 0.7154632 (406)\ttotal: 5m 16s\tremaining: 1m 8s\n",
      "420:\tlearn: 0.7956245\ttest: 0.7171118\tbest: 0.7181009 (414)\ttotal: 5m 24s\tremaining: 1m\n",
      "430:\tlearn: 0.7973835\ttest: 0.7181009\tbest: 0.7190900 (425)\ttotal: 5m 32s\tremaining: 53.2s\n",
      "440:\tlearn: 0.7988127\ttest: 0.7157929\tbest: 0.7190900 (425)\ttotal: 5m 40s\tremaining: 45.5s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7190900099\n",
      "bestIteration = 425\n",
      "\n",
      "Shrink model to first 426 iterations.\n",
      "0:\tlearn: 0.1363237\ttest: 0.1223211\tbest: 0.1223211 (0)\ttotal: 3.11s\tremaining: 25m 52s\n",
      "10:\tlearn: 0.2728672\ttest: 0.2453017\tbest: 0.2453017 (10)\ttotal: 34.5s\tremaining: 25m 33s\n",
      "20:\tlearn: 0.3181618\ttest: 0.2756347\tbest: 0.2756347 (20)\ttotal: 1m 6s\tremaining: 25m 8s\n",
      "30:\tlearn: 0.3634565\ttest: 0.3086053\tbest: 0.3086053 (30)\ttotal: 1m 38s\tremaining: 24m 42s\n",
      "40:\tlearn: 0.3990765\ttest: 0.3395978\tbest: 0.3395978 (40)\ttotal: 2m 10s\tremaining: 24m 15s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m machinery_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_machinery, eval_set\u001b[38;5;241m=\u001b[39m(X_val, y_val_machinery))\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Assembly 레이블에 대한 모델 훈련\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43massembly_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_assembly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_assembly\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\catboost\\core.py:5220\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5218\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5220\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5221\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5222\u001b[0m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\catboost\\core.py:2400\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2397\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2400\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2406\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2408\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2409\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\catboost\\core.py:1780\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4833\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4882\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# XGBoost 모델 학습 및 평가\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# CatBoost 분류기 준비\n",
    "machinery_model = CatBoostClassifier(\n",
    "    iterations=500,          # 반복 수 (트리 수)\n",
    "    depth=6,                 # 트리의 깊이\n",
    "    learning_rate=0.05,      # 학습률\n",
    "    loss_function='MultiClass',  # 손실 함수 설정\n",
    "    eval_metric='Accuracy',  # 평가 메트릭\n",
    "    random_seed=42,          # 난수 시드\n",
    "    od_type='Iter',          # 조기 종료 옵션\n",
    "    od_wait=20,              # 조기 종료를 위한 대기 단계\n",
    "    verbose=10               # 학습 과정에서 진행 상황 출력 간격\n",
    ")\n",
    "\n",
    "assembly_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    loss_function='MultiClass',\n",
    "    eval_metric='Accuracy',\n",
    "    random_seed=42,\n",
    "    od_type='Iter',\n",
    "    od_wait=20,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# Machinery 레이블에 대한 모델 훈련\n",
    "machinery_model.fit(X_train, y_train_machinery, eval_set=(X_val, y_val_machinery))\n",
    "\n",
    "# Assembly 레이블에 대한 모델 훈련\n",
    "assembly_model.fit(X_train, y_train_assembly, eval_set=(X_val, y_val_assembly))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "79ec5a06-43f6-4f61-abaf-a91b606adfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy for Machinery: 0.7309594460929772\n",
      "Validation Accuracy for Assembly: 0.5792944279591163\n"
     ]
    }
   ],
   "source": [
    "# 예측 및 평가\n",
    "y_pred_machinery = machinery_model.predict(X_val)\n",
    "y_pred_assembly = assembly_model.predict(X_val)\n",
    "\n",
    "accuracy_machinery = accuracy_score(y_val_machinery, y_pred_machinery)\n",
    "accuracy_assembly = accuracy_score(y_val_assembly, y_pred_assembly)\n",
    "\n",
    "print(f\"Validation Accuracy for Machinery: {accuracy_machinery}\")\n",
    "print(f\"Validation Accuracy for Assembly: {accuracy_assembly}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ce9496a-9c8e-4232-aa03-c3efbe2ffa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor size: torch.Size([9703, 200])\n",
      "y_train_machinery_tensor size: torch.Size([9703])\n",
      "y_train_assembly_tensor size: torch.Size([9703])\n",
      "X_val_tensor size: torch.Size([2426, 200])\n",
      "y_val_machinery_tensor size: torch.Size([2426])\n",
      "y_val_assembly_tensor size: torch.Size([2426])\n",
      "X_test_tensor size: torch.Size([3033, 200])\n",
      "y_test_machinery_tensor size: torch.Size([3033])\n",
      "y_test_assembly_tensor size: torch.Size([3033])\n"
     ]
    }
   ],
   "source": [
    "# 크기 출력 - tensor를 안붙이고 그냥 size를 달라고 하니까 못뱉어냄.\n",
    "print(f\"X_train_tensor size: {X_train_tensor.size()}\")\n",
    "print(f\"y_train_machinery_tensor size: {y_train_machinery_tensor.size()}\")\n",
    "print(f\"y_train_assembly_tensor size: {y_train_assembly_tensor.size()}\")\n",
    "\n",
    "print(f\"X_val_tensor size: {X_val_tensor.size()}\")\n",
    "print(f\"y_val_machinery_tensor size: {y_val_machinery_tensor.size()}\")\n",
    "print(f\"y_val_assembly_tensor size: {y_val_assembly_tensor.size()}\")\n",
    "\n",
    "print(f\"X_test_tensor size: {X_test_tensor.size()}\")\n",
    "print(f\"y_test_machinery_tensor size: {y_test_machinery_tensor.size()}\")\n",
    "print(f\"y_test_assembly_tensor size: {y_test_assembly_tensor.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ba940ca-cd72-438f-9599-9e542d05b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# TensorDataset 정의\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_machinery_tensor, y_train_assembly_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_machinery_tensor, y_val_assembly_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_machinery_tensor, y_test_assembly_tensor)\n",
    "\n",
    "# DataLoader 정의\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "21cef2cc-c498-4ce1-a15c-9992b2295fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SharedTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, dropout=0.3):\n",
    "        super(SharedTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, nhead=num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x.mean(dim=1)  # 평균을 취해 시퀀스 차원을 축소\n",
    "\n",
    "class MachineryHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(MachineryHead, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class AssemblyHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(AssemblyHead, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, machinery_classes, assembly_classes):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "        self.shared_transformer = SharedTransformer(input_dim, hidden_dim, num_heads, num_layers)\n",
    "        self.machinery_head = MachineryHead(hidden_dim, machinery_classes)\n",
    "        self.assembly_head = AssemblyHead(hidden_dim, assembly_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_features = self.shared_transformer(x)\n",
    "        machinery_output = self.machinery_head(shared_features)\n",
    "        assembly_output = self.assembly_head(shared_features)\n",
    "        return machinery_output, assembly_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "71205f68-8784-4e1e-92be-b58ad19c7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 256\n",
    "machinery_output_dim = len(np.unique(machinery_labels))  # 전체 클래스 개수\n",
    "assembly_output_dim = len(np.unique(assembly_labels))  # 전체 클래스 개수\n",
    "num_heads = 4\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13b7ebf6-ae95-443f-83ce-dea2bf3b7b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SharedTransformer(\n",
      "  (embedding): Linear(in_features=200, out_features=256, bias=True)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.3, inplace=False)\n",
      "        (dropout2): Dropout(p=0.3, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MachineryHead(\n",
      "  (fc): Linear(in_features=256, out_features=68, bias=True)\n",
      ")\n",
      "AssemblyHead(\n",
      "  (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      ")\n",
      "MultiOutputModel(\n",
      "  (shared_transformer): SharedTransformer(\n",
      "    (embedding): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.3, inplace=False)\n",
      "          (dropout2): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (machinery_head): MachineryHead(\n",
      "    (fc): Linear(in_features=256, out_features=68, bias=True)\n",
      "  )\n",
      "  (assembly_head): AssemblyHead(\n",
      "    (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 공유된 Transformer .\n",
    "shared_transformer = SharedTransformer(input_dim, hidden_dim, num_heads, num_layers).to(device)\n",
    "print(shared_transformer)\n",
    "\n",
    "# Machinery와 Assembly에 대한 개별 최종 레이어\n",
    "machinery_head = MachineryHead(hidden_dim, machinery_output_dim).to(device)\n",
    "print(machinery_head)\n",
    "\n",
    "assembly_head = AssemblyHead(hidden_dim, assembly_output_dim).to(device)\n",
    "print(assembly_head)\n",
    "\n",
    "model = MultiOutputModel(input_dim, hidden_dim, num_heads, num_layers, machinery_classes, assembly_classes).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d3387c4f-6f49-4d9c-974b-432eda8d1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 및 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "319b7074-2b7c-48ed-aa71-7da87a8dc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # 예시로 50 epoch 설정\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels_machinery, labels_assembly in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels_machinery = labels_machinery.to(device)\n",
    "            labels_assembly = labels_assembly.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs_machinery, outputs_assembly = model(inputs)\n",
    "            \n",
    "            loss_machinery = criterion(outputs_machinery, labels_machinery)\n",
    "            loss_assembly = criterion(outputs_assembly, labels_assembly)\n",
    "            loss = loss_machinery + loss_assembly\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "        # 검증 단계\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for inputs, labels_machinery, labels_assembly in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels_machinery = labels_machinery.to(device)\n",
    "                labels_assembly = labels_assembly.to(device)\n",
    "                outputs_machinery, outputs_assembly = model(inputs)\n",
    "                loss_machinery = criterion(outputs_machinery, labels_machinery)\n",
    "                loss_assembly = criterion(outputs_assembly, labels_assembly)\n",
    "                val_loss += (loss_machinery.item() + loss_assembly.item())\n",
    "            \n",
    "            print(f'Validation Loss: {val_loss/len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7e6b8190-db76-4843-b2ff-8e2f42b3d10a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x32 and 256x68)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[95], line 12\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[0;32m      9\u001b[0m labels_assembly \u001b[38;5;241m=\u001b[39m labels_assembly\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m outputs_machinery, outputs_assembly \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss_machinery \u001b[38;5;241m=\u001b[39m criterion(outputs_machinery, labels_machinery)\n\u001b[0;32m     15\u001b[0m loss_assembly \u001b[38;5;241m=\u001b[39m criterion(outputs_assembly, labels_assembly)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[90], line 45\u001b[0m, in \u001b[0;36mMultiOutputModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     44\u001b[0m     shared_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_transformer(x)\n\u001b[1;32m---> 45\u001b[0m     machinery_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmachinery_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     assembly_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massembly_head(shared_features)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m machinery_output, assembly_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[90], line 26\u001b[0m, in \u001b[0;36mMachineryHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x32 and 256x68)"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "52dbf625-2b93-41c4-b956-66115c275b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (9703, 200)\n",
      "y_train_machinery shape: (9703,)\n",
      "y_train_assembly shape: (9703,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train_machinery shape: {y_train_machinery.shape}')\n",
    "print(f'y_train_assembly shape: {y_train_assembly.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eef1ea-d477-496d-966a-0bdaad6f2e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc2528-3d20-46de-aae3-5c3e3fcb8d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf009e4d-8dd6-4e58-a3ad-df4efd1bdd23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ship)",
   "language": "python",
   "name": "ship"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
