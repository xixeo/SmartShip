{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de827958-8584-401f-9d62-1c3ecb4de85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9715e073-3876-4b8a-8d27-9987e232a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('filtered_30_filled_money.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39d5554b-b84a-4985-9202-f4f690d91c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13882 entries, 0 to 13881\n",
      "Data columns (total 32 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   청구서번호        13882 non-null  object \n",
      " 1   No.          13882 non-null  int64  \n",
      " 2   Subject      13872 non-null  object \n",
      " 3   Machinery    13882 non-null  object \n",
      " 4   Assembly     13882 non-null  object \n",
      " 5   청구품목         13882 non-null  object \n",
      " 6   Unnamed: 6   0 non-null      float64\n",
      " 7   Part No.1    13881 non-null  object \n",
      " 8   Part No.2    2430 non-null   object \n",
      " 9   청구량          13818 non-null  float64\n",
      " 10  견적           13698 non-null  object \n",
      " 11  견적수량         13818 non-null  float64\n",
      " 12  견적화폐         13882 non-null  object \n",
      " 13  견적단가         13882 non-null  float64\n",
      " 14  발주번호         13882 non-null  object \n",
      " 15  발주처          13882 non-null  object \n",
      " 16  발주           13882 non-null  object \n",
      " 17  발주수량         13818 non-null  float64\n",
      " 18  발주금액         13818 non-null  float64\n",
      " 19  D/T          12461 non-null  object \n",
      " 20  미입고 기간       998 non-null    object \n",
      " 21  창고입고         11867 non-null  object \n",
      " 22  창고입고수량       13882 non-null  int64  \n",
      " 23  Control No.  8571 non-null   object \n",
      " 24  입고창고         11867 non-null  object \n",
      " 25  창고출고         10595 non-null  object \n",
      " 26  창고출고수량       13882 non-null  int64  \n",
      " 27  출고선박         10595 non-null  object \n",
      " 28  출고운반선        10595 non-null  object \n",
      " 29  선박입고         3071 non-null   object \n",
      " 30  선박입고수량       13882 non-null  int64  \n",
      " 31  완료 여부        3065 non-null   object \n",
      "dtypes: float64(6), int64(4), object(22)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c43638-092f-4bb4-ad7b-15995cf0c80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['견적화폐'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94099074-a4c5-4800-877c-2d657cd73fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    text = re.sub(r'[^\\w\\s\\*/\\-\\+.,#&]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\b(사용금지|사)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_supplier_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r'coporation|coropration|coproration|corporration', 'corporation', name)\n",
    "    name = re.sub(r'\\(사용금지\\)', '', name)\n",
    "    name = re.sub(r'u\\.s\\.a', '_usa', name)\n",
    "    name = re.sub(r'\\.', '', name)\n",
    "    suffixes = r'(corporation|corp|company|co|incorporated|inc|limited|ltd|상사|공사|엔지니어링|주식회사|주|gmbh|pte ltd|llc)'\n",
    "    name = re.sub(suffixes, '', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bd270e-abd8-44ab-a486-48469d26853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리\n",
    "data['cleaned_item'] = data['청구품목'].apply(preprocess_text)\n",
    "data['cleaned_supplier'] = data['발주처'].apply(clean_supplier_name)\n",
    "data['combined_text'] = data['cleaned_item'].fillna('') + \" \" + data['Part No.1'].fillna('') + \" \" + data['cleaned_supplier'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3f5153d-187c-494f-bc74-1194a8f7c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_rates = {'USD': 1, 'KRW': 0.00078, 'EUR': 1.18, 'JPY': 0.0091}\n",
    "\n",
    "# usd기준해서 금액 통일함 \n",
    "data['converted_price'] = data.apply(lambda x: x['견적단가'] * exchange_rates[x['견적화폐']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b45129-3e52-490c-a953-21ef3b1756d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USD' 'KRW' 'EUR' 'JPY'] 0\n"
     ]
    }
   ],
   "source": [
    "print(data['견적화폐'].unique(), data['견적화폐'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c8df14-38bf-4da4-9c92-3f8a5305c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_ohe = OneHotEncoder(sparse_output=False) \n",
    "currency_encoded = currency_ohe.fit_transform(data[['견적화폐']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4655b8ab-ae1b-4280-b44b-1d0d2ae15695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 인코딩\n",
    "machinery_label_encoder = LabelEncoder()\n",
    "y_machinery = machinery_label_encoder.fit_transform(data['Machinery'])\n",
    "\n",
    "assembly_label_encoder = LabelEncoder()\n",
    "y_assembly = assembly_label_encoder.fit_transform(data['Assembly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8522452-bdfc-4c8a-9027-d460b64428bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_text shape: (13882,)\n",
      "currency_encoded shape: (13882, 4)\n",
      "converted_price shape: (13882,)\n",
      "X shape after concatenation: (13882, 6)\n",
      "X_train size: (10029, 6)\n",
      "X_val size: (1770, 6)\n",
      "X_test size: (2083, 6)\n"
     ]
    }
   ],
   "source": [
    "# train_test split 을 위해 하나로 모으고, 분할하고 다시 텍스트랑 추가피쳐로 분리해줄거임 \n",
    "\n",
    "# 1. 텍스트 + 추가 피처 결합\n",
    "X = np.concatenate([\n",
    "    data['combined_text'].values.reshape(-1, 1),  # 2차원 배열로 바꿔서 결합해줌 \n",
    "    currency_encoded, \n",
    "    data['converted_price'].values.reshape(-1, 1)  # 통일한단가\n",
    "], axis=1)\n",
    "\n",
    "X_train_val, X_test, y_train_val_machinery, y_test_machinery, y_train_val_assembly, y_test_assembly = train_test_split(\n",
    "    X, y_machinery, y_assembly, test_size=0.15, random_state=42, stratify=y_machinery)\n",
    "\n",
    "X_train, X_val, y_train_machinery, y_val_machinery, y_train_assembly, y_val_assembly = train_test_split(\n",
    "    X_train_val, y_train_val_machinery, y_train_val_assembly, test_size=0.15, random_state=42, stratify=y_train_val_machinery)\n",
    "\n",
    "# 크기 확인\n",
    "print(f\"combined_text shape: {data['combined_text'].shape}\")\n",
    "print(f\"currency_encoded shape: {currency_encoded.shape}\")\n",
    "print(f\"converted_price shape: {data['converted_price'].shape}\")\n",
    "print(f\"X shape after concatenation: {X.shape}\")\n",
    "\n",
    "print(f\"X_train size: {X_train.shape}\")\n",
    "print(f\"X_val size: {X_val.shape}\")\n",
    "print(f\"X_test size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1205a554-0603-4f20-b3b1-bbf075614ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_extra_features size: (10029, 5)\n",
      "val_extra_features size: (1770, 5)\n",
      "test_extra_features size: (2083, 5)\n"
     ]
    }
   ],
   "source": [
    "#텍스트분리\n",
    "train_combined_text = X_train[:, 0] \n",
    "val_combined_text = X_val[:, 0]\n",
    "test_combined_text = X_test[:, 0]\n",
    "\n",
    "train_extra_features = X_train[:, 1:]  # 이 부분에서 이미 2차원으로 분리됨\n",
    "val_extra_features = X_val[:, 1:]\n",
    "test_extra_features = X_test[:, 1:]\n",
    "\n",
    "# object타입이 섞여있다고 해서 astype float 명시해줌\n",
    "train_extra_features = np.nan_to_num(train_extra_features, nan=0.0).astype(float)\n",
    "val_extra_features = np.nan_to_num(val_extra_features, nan=0.0).astype(float)\n",
    "test_extra_features = np.nan_to_num(test_extra_features, nan=0.0).astype(float)\n",
    "\n",
    "# Torch Tensor로 변환 - 추가로 변환할 필요 없이 2차원 유지\n",
    "train_extra_features_tensor = torch.tensor(train_extra_features, dtype=torch.float32)  # 이미 2차원\n",
    "val_extra_features_tensor = torch.tensor(val_extra_features, dtype=torch.float32)\n",
    "test_extra_features_tensor = torch.tensor(test_extra_features, dtype=torch.float32)\n",
    "\n",
    "# 크기 확인\n",
    "print(f\"train_extra_features size: {train_extra_features.shape}\")\n",
    "print(f\"val_extra_features size: {val_extra_features.shape}\")\n",
    "print(f\"test_extra_features size: {test_extra_features.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ecdda32-f4f2-43c5-825a-dd17b34b404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_extra_features dtype: float64\n",
      "val_extra_features dtype: float64\n",
      "test_extra_features dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 데이터 타입 확인\n",
    "print(f\"train_extra_features dtype: {train_extra_features.dtype}\")\n",
    "print(f\"val_extra_features dtype: {val_extra_features.dtype}\")\n",
    "print(f\"test_extra_features dtype: {test_extra_features.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12affe13-a536-4714-913c-70912dd1abba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이저 (텍스트처리)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44558653-ff97-42cb-a192-c6b0dd9e8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X중 텍스트만 BERT 입력 형식으로 변환\n",
    "def encode_data(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "train_encodings = encode_data(train_combined_text)\n",
    "val_encodings = encode_data(val_combined_text)\n",
    "test_encodings = encode_data(test_combined_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65e97158-70b4-4ece-99f5-8bf3c78f476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 텍스트 인코딩 + 추가 피처 더해서 dataset 생성\n",
    "train_dataset = TensorDataset(\n",
    "    train_encodings['input_ids'],\n",
    "    train_encodings['attention_mask'],\n",
    "    train_extra_features_tensor,\n",
    "    torch.tensor(y_train_machinery),\n",
    "    torch.tensor(y_train_assembly)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    val_encodings['input_ids'],\n",
    "    val_encodings['attention_mask'],\n",
    "    val_extra_features_tensor,\n",
    "    torch.tensor(y_val_machinery),\n",
    "    torch.tensor(y_val_assembly)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    test_encodings['input_ids'],\n",
    "    test_encodings['attention_mask'],\n",
    "    test_extra_features_tensor,\n",
    "    torch.tensor(y_test_machinery),\n",
    "    torch.tensor(y_test_assembly)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c6fc79-7f45-4a76-b7ca-001707c64f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train size: (10029,)\n",
      "y_val size: (1770,)\n",
      "y_test size: (2083,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train size: {y_train_assembly.shape}\")\n",
    "print(f\"y_val size: {y_val_assembly.shape}\")\n",
    "print(f\"y_test size: {y_test_assembly.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79c5b89d-d987-4d05-9d72-e1ba20c3e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38a8452c-0350-448f-b5c5-641b8e512ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machinery 예측 후 Assembly 모델에 전달\n",
    "class BertWithTwoStages(nn.Module):\n",
    "    def __init__(self, num_machinery_labels, num_assembly_labels, extra_features_dim):\n",
    "        super(BertWithTwoStages, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.machinery_classifier = nn.Linear(self.bert.config.hidden_size + extra_features_dim, num_machinery_labels)\n",
    "        \n",
    "        self.assembly_hidden = nn.Linear(self.bert.config.hidden_size + extra_features_dim + num_machinery_labels, 512) \n",
    "\n",
    "        self.assembly_activation = nn.ReLU()  # 활성화 함수\n",
    "        self.assembly_dropout = nn.Dropout(0.3)  # 드롭아웃 추가\n",
    "        self.assembly_classifier = nn.Linear(512, num_assembly_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, extra_features):\n",
    "        # BERT에서 pooled_output 추출\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        # Machinery 예측\n",
    "        machinery_combined_features = torch.cat((pooled_output, extra_features), dim=1)\n",
    "        machinery_outputs = self.machinery_classifier(machinery_combined_features)\n",
    "\n",
    "        # Assembly 예측 (machinery 예측값 추가)\n",
    "        combined_features = torch.cat((pooled_output, extra_features, machinery_outputs), dim=1)\n",
    "        \n",
    "        assembly_hidden_output = self.assembly_hidden(combined_features)\n",
    "        assembly_hidden_output = self.assembly_activation(assembly_hidden_output)\n",
    "        assembly_hidden_output = self.assembly_dropout(assembly_hidden_output)\n",
    "        \n",
    "        assembly_outputs = self.assembly_classifier(assembly_hidden_output)\n",
    "    \n",
    "\n",
    "        return machinery_outputs, assembly_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3f8589d-d1e7-4fcd-9d91-0634e8e6249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62779f71-0ceb-44dd-be47-bf9b697296aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertWithTwoStages(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (machinery_classifier): Linear(in_features=773, out_features=62, bias=True)\n",
       "  (assembly_hidden): Linear(in_features=835, out_features=512, bias=True)\n",
       "  (assembly_activation): ReLU()\n",
       "  (assembly_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (assembly_classifier): Linear(in_features=512, out_features=209, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Machinery와 Assembly의 클래스 개수 확인\n",
    "num_machinery_labels = len(machinery_label_encoder.classes_)  # machinery 레이블의 클래스 수\n",
    "num_assembly_labels = len(assembly_label_encoder.classes_)    # assembly 레이블의 클래스 수\n",
    "\n",
    "# 모델 초기화\n",
    "model = BertWithTwoStages(num_machinery_labels=num_machinery_labels, num_assembly_labels=num_assembly_labels, extra_features_dim=5)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b4db600-e114-4be6-834e-5d153ba444b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 및 학습률 스케줄러 설정\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "772d3add-051f-4e5c-b5d0-c9c7ce008715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_mask, extra_features, y_machinery, y_assembly = [b.to(device) for b in batch]  # 두 개의 레이블\n",
    "        \n",
    "        # 레이블을 int64로 변환\n",
    "        y_machinery = y_machinery.to(torch.int64)\n",
    "        y_assembly = y_assembly.to(torch.int64)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        machinery_outputs, assembly_outputs = model(input_ids, attention_mask=attention_mask, extra_features=extra_features)\n",
    "\n",
    "        machinery_loss = loss_fn(machinery_outputs, y_machinery)\n",
    "        assembly_loss = loss_fn(assembly_outputs, y_assembly)\n",
    "        \n",
    "        loss = machinery_loss + assembly_loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9990e338-c952-4a12-ba6e-b5d960e884ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 평가 함수 - logits-62개짜리 각각의 자신감\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_correct_machinery = 0\n",
    "    total_correct_assembly = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids, attention_mask, extra_features, y_machinery, y_assembly = [b.to(device) for b in batch]\n",
    "            machinery_outputs, assembly_outputs = model(input_ids, attention_mask=attention_mask, extra_features=extra_features)\n",
    "\n",
    "            # Machinery 예측 정확도 계산\n",
    "            _, predicted_machinery = torch.max(F.softmax(machinery_outputs, dim=1), 1)\n",
    "            total_correct_machinery += (predicted_machinery == y_machinery).sum().item()\n",
    "\n",
    "            # Assembly 예측 정확도 계산\n",
    "            _, predicted_assembly = torch.max(F.softmax(assembly_outputs, dim=1), 1)\n",
    "            total_correct_assembly += (predicted_assembly == y_assembly).sum().item()\n",
    "\n",
    "            total_samples += y_machinery.size(0)\n",
    "\n",
    "    # 두 개의 정확도 반환\n",
    "    accuracy_machinery = total_correct_machinery / total_samples\n",
    "    accuracy_assembly = total_correct_assembly / total_samples\n",
    "    return accuracy_machinery, accuracy_assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "381695c8-99af-44f7-b5d6-425661364fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1254 [00:00<?, ?it/s]C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:56<00:00,  5.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [01:05<00:00, 19.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 24.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Train Loss: 34.9749, Train Machinery Acc: 0.5485, Train Assembly Acc: 0.1576\n",
      "Val Machinery Acc: 0.5328, Val Assembly Acc: 0.1469\n",
      "Test Machinery Acc: 0.5425, Test Assembly Acc: 0.1637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:56<00:00,  5.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [01:05<00:00, 19.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 24.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "Train Loss: 22.3900, Train Machinery Acc: 0.6330, Train Assembly Acc: 0.2092\n",
      "Val Machinery Acc: 0.6192, Val Assembly Acc: 0.1893\n",
      "Test Machinery Acc: 0.6284, Test Assembly Acc: 0.1959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████▍                                                     | 403/1254 [01:16<02:41,  5.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 두 개의 평가 함수 호출\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     train_acc_machinery, train_acc_assembly \u001b[38;5;241m=\u001b[39m evaluate(model, train_loader, device)\n",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[0;32m      3\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m----> 5\u001b[0m     input_ids, attention_mask, extra_features, y_machinery, y_assembly \u001b[38;5;241m=\u001b[39m [\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]  \u001b[38;5;66;03m# 두 개의 레이블\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# 레이블을 int64로 변환\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     y_machinery \u001b[38;5;241m=\u001b[39m y_machinery\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # 두 개의 평가 함수 호출\n",
    "    train_acc_machinery, train_acc_assembly = evaluate(model, train_loader, device)\n",
    "    val_acc_machinery, val_acc_assembly = evaluate(model, val_loader, device)\n",
    "    test_acc_machinery, test_acc_assembly = evaluate(model, test_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Machinery Acc: {train_acc_machinery:.4f}, Train Assembly Acc: {train_acc_assembly:.4f}\")\n",
    "    print(f\"Val Machinery Acc: {val_acc_machinery:.4f}, Val Assembly Acc: {val_acc_assembly:.4f}\")\n",
    "    print(f\"Test Machinery Acc: {test_acc_machinery:.4f}, Test Assembly Acc: {test_acc_assembly:.4f}\")\n",
    "\n",
    "# 최종 테스트 성능 평가\n",
    "final_test_acc_machinery, final_test_acc_assembly = evaluate(model, test_loader, device)\n",
    "print(f\"Final Test Machinery Accuracy: {final_test_acc_machinery:.4f}\")\n",
    "print(f\"Final Test Assembly Accuracy: {final_test_acc_assembly:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc85d9f-f713-4a21-99f1-210835688c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f114ac-677d-467d-bcd5-0e4d2510f1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ship)",
   "language": "python",
   "name": "ship"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
