{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de827958-8584-401f-9d62-1c3ecb4de85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9715e073-3876-4b8a-8d27-9987e232a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('filtered_30_filled_money.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94099074-a4c5-4800-877c-2d657cd73fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    text = re.sub(r'[^\\w\\s\\*/\\-\\+.,#&]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\b(사용금지|사)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_supplier_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r'coporation|coropration|coproration|corporration', 'corporation', name)\n",
    "    name = re.sub(r'\\(사용금지\\)', '', name)\n",
    "    name = re.sub(r'u\\.s\\.a', '_usa', name)\n",
    "    name = re.sub(r'\\.', '', name)\n",
    "    suffixes = r'(corporation|corp|company|co|incorporated|inc|limited|ltd|상사|공사|엔지니어링|주식회사|주|gmbh|pte ltd|llc)'\n",
    "    name = re.sub(suffixes, '', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bd270e-abd8-44ab-a486-48469d26853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리\n",
    "data['cleaned_item'] = data['청구품목'].apply(preprocess_text)\n",
    "data['cleaned_supplier'] = data['발주처'].apply(clean_supplier_name)\n",
    "data['combined_text'] = data['cleaned_item'].fillna('') + \" \" + data['Part No.1'].fillna('') + \" \" + data['cleaned_supplier'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f5153d-187c-494f-bc74-1194a8f7c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_rates = {'USD': 1, 'KRW': 0.00078, 'EUR': 1.18, 'JPY': 0.0091}\n",
    "\n",
    "# usd기준해서 금액 통일함 \n",
    "data['converted_price'] = data.apply(lambda x: x['견적단가'] * exchange_rates[x['견적화폐']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b45129-3e52-490c-a953-21ef3b1756d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USD' 'KRW' 'EUR' 'JPY'] 0\n"
     ]
    }
   ],
   "source": [
    "print(data['견적화폐'].unique(), data['견적화폐'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c8df14-38bf-4da4-9c92-3f8a5305c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_ohe = OneHotEncoder(sparse_output=False) \n",
    "currency_encoded = currency_ohe.fit_transform(data[['견적화폐']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4655b8ab-ae1b-4280-b44b-1d0d2ae15695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 인코딩\n",
    "machinery_label_encoder = LabelEncoder()\n",
    "y_machinery= machinery_label_encoder.fit_transform(data['Machinery'])\n",
    "\n",
    "assembly_label_encoder = LabelEncoder()\n",
    "y_assembly = assembly_label_encoder.fit_transform(data['Assembly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8522452-bdfc-4c8a-9027-d460b64428bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_text shape: (13882,)\n",
      "currency_encoded shape: (13882, 4)\n",
      "converted_price shape: (13882,)\n",
      "X shape after concatenation: (13882, 6)\n",
      "X_train size: (10029, 6)\n",
      "X_val size: (1770, 6)\n",
      "X_test size: (2083, 6)\n"
     ]
    }
   ],
   "source": [
    "# train_test split 을 위해 하나로 모으고, 분할하고 다시 텍스트랑 추가피쳐로 분리해줄거임 \n",
    "\n",
    "# 1. 텍스트 + 추가 피처 결합\n",
    "X = np.concatenate([\n",
    "    data['combined_text'].values.reshape(-1, 1),  # 2차원 배열로 바꿔서 결합해줌 \n",
    "    currency_encoded, \n",
    "    data['converted_price'].values.reshape(-1, 1)  # 통일한단가\n",
    "], axis=1)\n",
    "\n",
    "X_train_val, X_test, y_train_val_machinery, y_test_machinery, y_train_val_assembly, y_test_assembly = train_test_split(\n",
    "    X, y_machinery, y_assembly, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=y_assembly)  # stratify는 주로 메인 레이블 기준으로 설정\n",
    "\n",
    "X_train, X_val, y_train_machinery, y_val_machinery, y_train_assembly, y_val_assembly = train_test_split(\n",
    "    X_train_val, y_train_val_machinery, y_train_val_assembly, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=y_train_val_assembly)  # 다시 stratify 기준으로 설정\n",
    "\n",
    "# 크기 확인\n",
    "print(f\"combined_text shape: {data['combined_text'].shape}\")\n",
    "print(f\"currency_encoded shape: {currency_encoded.shape}\")\n",
    "print(f\"converted_price shape: {data['converted_price'].shape}\")\n",
    "print(f\"X shape after concatenation: {X.shape}\")\n",
    "\n",
    "print(f\"X_train size: {X_train.shape}\")\n",
    "print(f\"X_val size: {X_val.shape}\")\n",
    "print(f\"X_test size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1205a554-0603-4f20-b3b1-bbf075614ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트분리\n",
    "train_combined_text = X_train[:, 0] \n",
    "val_combined_text = X_val[:, 0]\n",
    "test_combined_text = X_test[:, 0]\n",
    "\n",
    "train_extra_features = X_train[:, 1:]  # 이 부분에서 이미 2차원으로 분리됨\n",
    "val_extra_features = X_val[:, 1:]\n",
    "test_extra_features = X_test[:, 1:]\n",
    "\n",
    "# object타입이 섞여있다고 해서 astype float 명시해줌\n",
    "train_extra_features = np.nan_to_num(train_extra_features, nan=0.0).astype(float)\n",
    "val_extra_features = np.nan_to_num(val_extra_features, nan=0.0).astype(float)\n",
    "test_extra_features = np.nan_to_num(test_extra_features, nan=0.0).astype(float)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "train_extra_features = scaler.fit_transform(train_extra_features)\n",
    "val_extra_features = scaler.transform(val_extra_features)\n",
    "test_extra_features = scaler.transform(test_extra_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ecdda32-f4f2-43c5-825a-dd17b34b404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# 클러스터링을 통한 추가 피처 생성\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "train_clusters = kmeans.fit_predict(train_extra_features[:, -1].reshape(-1, 1))  # converted_price 컬럼 클러스터링\n",
    "val_clusters = kmeans.predict(val_extra_features[:, -1].reshape(-1, 1))\n",
    "test_clusters = kmeans.predict(test_extra_features[:, -1].reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14fe5901-9c93-4782-8826-b73337fdb0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터 레이블을 원-핫 인코딩\n",
    "cluster_ohe = OneHotEncoder(sparse_output=False)\n",
    "train_clusters_encoded = cluster_ohe.fit_transform(train_clusters.reshape(-1, 1))\n",
    "val_clusters_encoded = cluster_ohe.transform(val_clusters.reshape(-1, 1))\n",
    "test_clusters_encoded = cluster_ohe.transform(test_clusters.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6793518f-4c42-43bf-b8e6-583d35b3b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 피처에 클러스터 인코딩 결합\n",
    "train_extra_features = np.concatenate([train_extra_features, train_clusters_encoded], axis=1)\n",
    "val_extra_features = np.concatenate([val_extra_features, val_clusters_encoded], axis=1)\n",
    "test_extra_features = np.concatenate([test_extra_features, test_clusters_encoded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e1270e8-9b60-4454-9851-b0c1d80e44fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9292\\2767559813.py:4: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  train_extra_features_tensor = torch.tensor(train_extra_features, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Torch Tensor로 변환 후 디바이스 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_extra_features_tensor = torch.tensor(train_extra_features, dtype=torch.float32).to(device)\n",
    "val_extra_features_tensor = torch.tensor(val_extra_features, dtype=torch.float32).to(device)\n",
    "test_extra_features_tensor = torch.tensor(test_extra_features, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12affe13-a536-4714-913c-70912dd1abba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이저 (텍스트처리)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44558653-ff97-42cb-a192-c6b0dd9e8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X중 텍스트만 BERT 입력 형식으로 변환\n",
    "def encode_data(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "train_encodings = encode_data(train_combined_text)\n",
    "val_encodings = encode_data(val_combined_text)\n",
    "test_encodings = encode_data(test_combined_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65e97158-70b4-4ece-99f5-8bf3c78f476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_machinery_dataset = TensorDataset(\n",
    "    train_encodings['input_ids'],\n",
    "    train_encodings['attention_mask'],\n",
    "    train_extra_features_tensor,\n",
    "    torch.tensor(y_train_machinery, dtype=torch.long).to(device)  # Machinery 레이블\n",
    ")\n",
    "\n",
    "val_machinery_dataset = TensorDataset(\n",
    "    val_encodings['input_ids'],\n",
    "    val_encodings['attention_mask'],\n",
    "    val_extra_features_tensor,\n",
    "    torch.tensor(y_val_machinery, dtype=torch.long).to(device)\n",
    ")\n",
    "\n",
    "test_machinery_dataset = TensorDataset(\n",
    "    test_encodings['input_ids'],\n",
    "    test_encodings['attention_mask'],\n",
    "    test_extra_features_tensor,\n",
    "    torch.tensor(y_test_machinery, dtype=torch.long).to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05c6fc79-7f45-4a76-b7ca-001707c64f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train size: (10029,)\n",
      "y_val size: (1770,)\n",
      "y_test size: (2083,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train size: {y_train_machinery.shape}\")\n",
    "print(f\"y_val size: {y_val_machinery.shape}\")\n",
    "print(f\"y_test size: {y_test_machinery.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79c5b89d-d987-4d05-9d72-e1ba20c3e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DataLoader 생성\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader_machinery = DataLoader(train_machinery_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader_machinery  = DataLoader(val_machinery_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader_machinery = DataLoader(test_machinery_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38a8452c-0350-448f-b5c5-641b8e512ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMachinery(nn.Module):\n",
    "    def __init__(self, num_machinery_labels, extra_features_dim):\n",
    "        super(BertForMachinery, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size + extra_features_dim, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.machinery_classifier = nn.Linear(256, num_machinery_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, extra_features):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        if extra_features.dim() == 1:\n",
    "            extra_features = extra_features.unsqueeze(1)\n",
    "        \n",
    "        machinery_combined_features = torch.cat((pooled_output, extra_features), dim=1)\n",
    "        x = self.fc1(machinery_combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        machinery_outputs = self.machinery_classifier(x)\n",
    "        \n",
    "        return machinery_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3f8589d-d1e7-4fcd-9d91-0634e8e6249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62779f71-0ceb-44dd-be47-bf9b697296aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMachinery(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=778, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (machinery_classifier): Linear(in_features=256, out_features=62, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "machinery_model = BertForMachinery(num_machinery_labels=len(machinery_label_encoder.classes_), extra_features_dim=10) \n",
    "machinery_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b4db600-e114-4be6-834e-5d153ba444b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 및 학습률 스케줄러 설정\n",
    "optimizer_machinery = AdamW(machinery_model.parameters(), lr=2e-5)\n",
    "loss_fn_machinery=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "772d3add-051f-4e5c-b5d0-c9c7ce008715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_machinery(model, dataloader, optimizer, device, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_mask, extra_features, labels = [b.to(device) for b in batch]  # 순서 수정\n",
    "        \n",
    "        if labels.dim() > 1:\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, extra_features=extra_features)\n",
    "        labels = labels.to(torch.int64)  # CrossEntropyLoss에 맞게 변환\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9990e338-c952-4a12-ba6e-b5d960e884ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 평가 함수 - logits-62개짜리 각각의 자신감\n",
    "def evaluate_machinery(model, dataloader, device, loss_fn_machinery):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    machinery_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids, attention_mask, extra_features, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # 모델 예측\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, extra_features=extra_features)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = loss_fn_machinery(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)  # softmax 없이 직접 logits에서 최대값 클래스 예측\n",
    "            \n",
    "            # 예측값을 저장\n",
    "            machinery_predictions.append(predicted.cpu().numpy())  # 리스트에 추가\n",
    "            \n",
    "            # 정확도 계산\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    machinery_predictions = np.concatenate(machinery_predictions, axis=0)  \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, accuracy, machinery_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a1505a3-0786-4beb-82b8-b897c930f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "381695c8-99af-44f7-b5d6-425661364fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/627 [00:00<?, ?it/s]C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:07<00:00,  3.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:11<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 2.2436, Val Loss: 1.4900, Val Acc: 0.6232, Test Acc: 0.6207\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:11<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 1.3356, Val Loss: 1.1412, Val Acc: 0.6989, Test Acc: 0.6961\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:11<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 1.0550, Val Loss: 0.9203, Val Acc: 0.7412, Test Acc: 0.7408\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:11<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.8936, Val Loss: 0.8294, Val Acc: 0.7655, Test Acc: 0.7571\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:11<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.7785, Val Loss: 0.7455, Val Acc: 0.7915, Test Acc: 0.7916\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:11<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.6703, Val Loss: 0.7176, Val Acc: 0.7972, Test Acc: 0.8008\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.5998, Val Loss: 0.6557, Val Acc: 0.8141, Test Acc: 0.8133\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.90it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.5363, Val Loss: 0.6060, Val Acc: 0.8209, Test Acc: 0.8147\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.4834, Val Loss: 0.5831, Val Acc: 0.8186, Test Acc: 0.8238\n",
      "Trigger Times (Machinery): 1\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.4433, Val Loss: 0.5829, Val Acc: 0.8249, Test Acc: 0.8272\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.4091, Val Loss: 0.5549, Val Acc: 0.8328, Test Acc: 0.8382\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.3821, Val Loss: 0.5626, Val Acc: 0.8328, Test Acc: 0.8243\n",
      "Trigger Times (Machinery): 1\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.3660, Val Loss: 0.5904, Val Acc: 0.8328, Test Acc: 0.8320\n",
      "Trigger Times (Machinery): 2\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.3404, Val Loss: 0.5610, Val Acc: 0.8367, Test Acc: 0.8368\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.3314, Val Loss: 0.5801, Val Acc: 0.8418, Test Acc: 0.8401\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.3042, Val Loss: 0.5583, Val Acc: 0.8424, Test Acc: 0.8421\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.2983, Val Loss: 0.6133, Val Acc: 0.8362, Test Acc: 0.8358\n",
      "Trigger Times (Machinery): 1\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.2847, Val Loss: 0.5576, Val Acc: 0.8407, Test Acc: 0.8406\n",
      "Trigger Times (Machinery): 2\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.2753, Val Loss: 0.5670, Val Acc: 0.8441, Test Acc: 0.8425\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 627/627 [03:05<00:00,  3.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 111/111 [00:12<00:00,  8.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.2764, Val Loss: 0.5816, Val Acc: 0.8458, Test Acc: 0.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [00:10<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy (Machinery): 0.8387\n"
     ]
    }
   ],
   "source": [
    "# Machinery 모델 학습 실행\n",
    "num_epochs = 20\n",
    "best_val_acc_machinery = 0\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Machinery 모델 학습\n",
    "    train_loss_machinery = train_machinery(\n",
    "        machinery_model, \n",
    "        train_loader_machinery, \n",
    "        optimizer_machinery, \n",
    "        device, \n",
    "        loss_fn_machinery\n",
    "    )\n",
    "    \n",
    "    # Machinery 모델 평가\n",
    "    val_loss_machinery, val_acc_machinery, val_preds_machinery = evaluate_machinery(\n",
    "        machinery_model, \n",
    "        val_loader_machinery, \n",
    "        device, \n",
    "        loss_fn_machinery\n",
    "    )\n",
    "    test_loss_machinery, test_acc_machinery, test_preds_machinery = evaluate_machinery(\n",
    "        machinery_model, \n",
    "        test_loader_machinery, \n",
    "        device, \n",
    "        loss_fn_machinery\n",
    "    )\n",
    "    \n",
    "    print(f\"Machinery - Train Loss: {train_loss_machinery:.4f}, Val Loss: {val_loss_machinery:.4f}, Val Acc: {val_acc_machinery:.4f}, Test Acc: {test_acc_machinery:.4f}\")\n",
    "    \n",
    "    # Early Stopping for Machinery\n",
    "    if val_acc_machinery > best_val_acc_machinery:\n",
    "        best_val_acc_machinery = val_acc_machinery\n",
    "        trigger_times = 0\n",
    "        torch.save(machinery_model.state_dict(), \"best_machinery_model.pth\")\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times (Machinery): {trigger_times}\")\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping for Machinery!\")\n",
    "            break\n",
    "\n",
    "# 최종 테스트 성능 평가\n",
    "final_test_loss_machinery, final_test_acc_machinery, final_machinery_predictions = evaluate_machinery(\n",
    "    machinery_model, \n",
    "    test_loader_machinery, \n",
    "    device, \n",
    "    loss_fn_machinery\n",
    ")\n",
    "print(f\"Final Test Accuracy (Machinery): {final_test_acc_machinery:.4f}\")\n",
    "torch.save(machinery_model.state_dict(), \"best_machinery_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aea936-31d1-4027-8cad-f4c5baa664b7",
   "metadata": {},
   "source": [
    "### Feature Extraction: 이미 학습된 모델을 고정(freeze)하고, 그 모델이 생성한 특징을 새로운 모델의 입력으로 사용함\n",
    "\n",
    "> 프로토타입-1개 최상위 예측 가져오도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3fa58e3-c59b-4f5a-a8f3-4d0fefab3977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9292\\3052808221.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  machinery_model.load_state_dict(torch.load(\"best_machinery_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "# Machinery 모델 로드 및 가중치 고정\n",
    "num_machinery_labels = len(np.unique(y_machinery))\n",
    "\n",
    "machinery_model = BertForMachinery(\n",
    "    num_machinery_labels=len(machinery_label_encoder.classes_), \n",
    "    extra_features_dim=train_extra_features.shape[1]\n",
    ").to(device)\n",
    "\n",
    "machinery_model.load_state_dict(torch.load(\"best_machinery_model.pth\"))\n",
    "machinery_model.eval()\n",
    "\n",
    "for param in machinery_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81db3137-4cdc-4924-a471-8ba12fe32996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_1_machinery_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    top_1_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Generating Top-1 Machinery Predictions\"):\n",
    "            input_ids, attention_mask, extra_features = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, extra_features=extra_features)\n",
    "            \n",
    "            # 상위 1개의 예측값 가져오기\n",
    "            _, top_1_predicted = torch.max(outputs, dim=1)\n",
    "            top_1_predictions.append(top_1_predicted.cpu())  # CPU로 이동 후 리스트에 추가\n",
    "    return torch.cat(top_1_predictions, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46aa5ad1-42b1-48a7-9d10-d5b7876ae3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d97b551-bc6e-4f79-87cf-c21fe4916a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Top-1 Machinery Predictions:   0%|                                                  | 0/627 [00:00<?, ?it/s]C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Generating Top-1 Machinery Predictions: 100%|████████████████████████████████████████| 627/627 [01:11<00:00,  8.76it/s]\n",
      "Generating Top-1 Machinery Predictions: 100%|████████████████████████████████████████| 111/111 [00:12<00:00,  8.95it/s]\n",
      "Generating Top-1 Machinery Predictions: 100%|████████████████████████████████████████| 131/131 [00:11<00:00, 11.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# 예측 생성 (상위 1개)\n",
    "train_machinery_predictions_top_1 = get_top_1_machinery_predictions(\n",
    "    machinery_model, \n",
    "    DataLoader(TensorDataset(\n",
    "        train_encodings['input_ids'], \n",
    "        train_encodings['attention_mask'], \n",
    "        train_extra_features_tensor\n",
    "    ), batch_size=16, shuffle=False)\n",
    ")\n",
    "\n",
    "val_machinery_predictions_top_1 = get_top_1_machinery_predictions(\n",
    "    machinery_model, \n",
    "    DataLoader(TensorDataset(\n",
    "        val_encodings['input_ids'], \n",
    "        val_encodings['attention_mask'], \n",
    "        val_extra_features_tensor\n",
    "    ), batch_size=16, shuffle=False)\n",
    ")\n",
    "\n",
    "test_machinery_predictions_top_1 = get_top_1_machinery_predictions(\n",
    "    machinery_model, \n",
    "    DataLoader(TensorDataset(\n",
    "        test_encodings['input_ids'], \n",
    "        test_encodings['attention_mask'], \n",
    "        test_extra_features_tensor\n",
    "    ), batch_size=16, shuffle=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39898b51-e4a3-4ae6-ade0-a8fa20d9cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_machinery_predictions_top_1 = train_machinery_predictions_top_1.unsqueeze(1)\n",
    "val_machinery_predictions_top_1 = val_machinery_predictions_top_1.unsqueeze(1)\n",
    "test_machinery_predictions_top_1 = test_machinery_predictions_top_1.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4fdd7aa-9be2-4b62-9f6a-849178324a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10029, 1])\n",
      "torch.Size([1770, 1])\n",
      "torch.Size([2083, 1])\n"
     ]
    }
   ],
   "source": [
    "# Machinery 예측 결과의 크기를 확인\n",
    "print(train_machinery_predictions_top_1.shape)\n",
    "print(val_machinery_predictions_top_1.shape)\n",
    "print(test_machinery_predictions_top_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af021bb3-fe0a-49c9-bba1-ece8f6682b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support 30개 이하 Assembly 클래스: {87: 27, 152: 26, 39: 22, 208: 26, 104: 29, 31: 26, 113: 28, 83: 26, 35: 30, 156: 22, 202: 28, 114: 25, 28: 29, 101: 23, 177: 27, 207: 26, 4: 29, 118: 30, 89: 29, 42: 23, 121: 29, 43: 26, 184: 28, 55: 30, 77: 23, 86: 29, 201: 26, 38: 22, 50: 26, 110: 22, 19: 30, 193: 23, 47: 25, 26: 22, 136: 28, 197: 30, 12: 29, 30: 24, 117: 25, 97: 23, 145: 28, 131: 24, 7: 27, 206: 25, 67: 28, 107: 26, 99: 30, 137: 25, 109: 26, 51: 27, 150: 23, 119: 26, 203: 27, 70: 22, 165: 26, 1: 29, 163: 26, 166: 26, 100: 27, 162: 22, 65: 22, 146: 30, 41: 29, 81: 24, 56: 22, 139: 24, 24: 23, 173: 26, 3: 26, 20: 22, 183: 26, 37: 24, 103: 25, 73: 26, 45: 25}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# 클래스별 데이터 수를 계산\n",
    "class_sample_count = Counter(y_train_assembly)\n",
    "\n",
    "# 소수 클래스 (30개 이하인 클래스)만 오버샘플링\n",
    "small_class_sample_count = {cls: count for cls, count in class_sample_count.items() if count <= 30}\n",
    "print(f\"Support 30개 이하 Assembly 클래스: {small_class_sample_count}\")\n",
    "\n",
    "# 가중치 계산 (소수 클래스는 높은 가중치)\n",
    "weights = np.ones_like(y_train_assembly, dtype=np.float32)\n",
    "for cls in small_class_sample_count:\n",
    "    class_indices = np.where(y_train_assembly == cls)[0]\n",
    "    weights[class_indices] = 1. / small_class_sample_count[cls]  # 소수 클래스의 가중치 부여\n",
    "\n",
    "# WeightedRandomSampler 생성\n",
    "sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "611fbef1-7449-455a-9b68-cc97c4041a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_assembly_dataset = TensorDataset(\n",
    "    train_encodings['input_ids'],\n",
    "    train_encodings['attention_mask'],\n",
    "    train_extra_features_tensor,\n",
    "    train_machinery_predictions_top_1.unsqueeze(1),  # 상위 1개의 Machinery 예측 추가\n",
    "    torch.tensor(y_train_assembly, dtype=torch.long).to(device)  # Assembly 레이블\n",
    ")\n",
    "\n",
    "val_assembly_dataset = TensorDataset(\n",
    "    val_encodings['input_ids'],\n",
    "    val_encodings['attention_mask'],\n",
    "    val_extra_features_tensor,\n",
    "    val_machinery_predictions_top_1.unsqueeze(1),  # 상위 1개의 Machinery 예측 추가\n",
    "    torch.tensor(y_val_assembly, dtype=torch.long).to(device)\n",
    ")\n",
    "\n",
    "test_assembly_dataset = TensorDataset(\n",
    "    test_encodings['input_ids'],\n",
    "    test_encodings['attention_mask'],\n",
    "    test_extra_features_tensor,\n",
    "    test_machinery_predictions_top_1.unsqueeze(1),  # 상위 1개의 Machinery 예측 추가\n",
    "    torch.tensor(y_test_assembly, dtype=torch.long).to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4516fcaa-2013-4153-b5e0-c3c87c5f19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "batch_size = 16\n",
    "train_loader_assembly = DataLoader(train_assembly_dataset, batch_size=batch_size, sampler=sampler)\n",
    "val_loader_assembly  = DataLoader(val_assembly_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader_assembly = DataLoader(test_assembly_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c5489b7-800a-4735-9aa6-55039e95e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_assembly_labels = len(assembly_label_encoder.classes_)\n",
    "machinery_output_dim = 62 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28355938-0a2c-4317-bdef-6fe2504aa27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssemblyModel(nn.Module):\n",
    "    def __init__(self, num_assembly_labels, extra_features_dim, machinery_output_dim):\n",
    "        super(AssemblyModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # fc1의 출력 차원을 128로 줄임\n",
    "        self.fc1 = nn.Linear(840, 128)\n",
    "        self.batchnorm = nn.BatchNorm1d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout 비율을 0.6으로 증가\n",
    "        self.fc2 = nn.Linear(128, num_assembly_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, machinery_outputs, extra_features):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_outputs.pooler_output\n",
    "        \n",
    "        # combined_features 크기 = (batch_size, 840)\n",
    "        combined_features = torch.cat((pooled_output, machinery_outputs, extra_features), dim=1)\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        assembly_outputs = self.fc2(x)\n",
    "        \n",
    "        return assembly_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbe2d3a9-1037-49a1-a980-2dadefc841b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44b1f66b-0ada-4bac-8def-d65fd51cda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembly 모델 초기화\n",
    "num_assembly_labels = len(np.unique(y_assembly))\n",
    "machinery_output_dim = 1  \n",
    "\n",
    "assembly_model = AssemblyModel(\n",
    "    num_assembly_labels=num_assembly_labels, \n",
    "    extra_features_dim=train_extra_features.shape[1], \n",
    "    machinery_output_dim=machinery_output_dim\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "884e4789-a0da-4a86-bc33-ff81287eb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# 일반적인 CrossEntropyLoss 사용\n",
    "loss_fn_assembly = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Optimizer 설정\n",
    "optimizer_assembly = AdamW(assembly_model.parameters(), lr=2e-5, weight_decay=0.01) \n",
    "scheduler = StepLR(optimizer_assembly, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97840c80-15a1-4f33-93ef-3f88c6d10b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec52308b-f51a-452d-9bcb-5ecc80e826cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMachinery(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=778, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (machinery_classifier): Linear(in_features=256, out_features=62, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "assembly_model.to(device)\n",
    "machinery_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4d65cc0-a1b1-4143-aa6c-25ae3d5ec897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembly 모델 학습 및 평가 함수 정의\n",
    "def train_assembly(model, machinery_model, dataloader, optimizer, device, loss_fn):\n",
    "    model.train()\n",
    "    machinery_model.eval()  # Machinery 모델은 학습되지 않으므로 평가 모드로 설정\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training Assembly\"):\n",
    "        input_ids, attention_mask, extra_features, machinery_predictions, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        extra_features = extra_features.to(device)\n",
    "        machinery_predictions = machinery_predictions.to(device)  # 이미 Machinery 예측을 포함\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Machinery 모델의 예측을 사용하여 Assembly 모델 입력 준비\n",
    "        with torch.no_grad():\n",
    "            machinery_outputs = machinery_model(input_ids, attention_mask, extra_features)\n",
    "        \n",
    "        # Assembly 모델 예측\n",
    "        outputs = model(input_ids, attention_mask, machinery_outputs, extra_features)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df772767-a7b4-48de-b958-3f7df967744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_assembly(model, machinery_model, dataloader, device):\n",
    "    model.eval()\n",
    "    machinery_model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating Assembly\"):\n",
    "            input_ids, attention_mask, extra_features, machinery_predictions, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            extra_features = extra_features.to(device)\n",
    "            machinery_predictions = machinery_predictions.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Machinery 모델의 예측 사용\n",
    "            machinery_outputs = machinery_model(input_ids, attention_mask, extra_features)\n",
    "            \n",
    "            # Assembly 모델 예측\n",
    "            outputs = model(input_ids, attention_mask, machinery_outputs, extra_features)\n",
    "            \n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(probs, 1)\n",
    "            \n",
    "            all_predictions.append(predicted.cpu().numpy())\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    return accuracy, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f60ae27e-61d9-4744-9a82-1afe4b909f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1a2420e-989b-463d-adef-b0ab856965ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Assembly:  71%|███████████████████████████████████████████                  | 443/627 [03:08<01:18,  2.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Assembly 모델 학습\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_loss_assembly \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_assembly\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43massembly_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachinery_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader_assembly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_assembly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn_assembly\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Assembly 모델 평가\u001b[39;00m\n\u001b[0;32m     21\u001b[0m train_acc_assembly, _ \u001b[38;5;241m=\u001b[39m evaluate_assembly(\n\u001b[0;32m     22\u001b[0m     assembly_model, \n\u001b[0;32m     23\u001b[0m     machinery_model, \n\u001b[0;32m     24\u001b[0m     train_loader_assembly, \n\u001b[0;32m     25\u001b[0m     device\n\u001b[0;32m     26\u001b[0m )\n",
      "Cell \u001b[1;32mIn[50], line 23\u001b[0m, in \u001b[0;36mtrain_assembly\u001b[1;34m(model, machinery_model, dataloader, optimizer, device, loss_fn)\u001b[0m\n\u001b[0;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask, machinery_outputs, extra_features)\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m---> 23\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assembly 모델 학습 실행\n",
    "num_epochs = 20\n",
    "best_val_acc_assembly = 0\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Assembly 모델 학습\n",
    "    train_loss_assembly = train_assembly(\n",
    "        assembly_model, \n",
    "        machinery_model, \n",
    "        train_loader_assembly, \n",
    "        optimizer_assembly, \n",
    "        device, \n",
    "        loss_fn_assembly\n",
    "    )\n",
    "    \n",
    "    # Assembly 모델 평가\n",
    "    train_acc_assembly, _ = evaluate_assembly(\n",
    "        assembly_model, \n",
    "        machinery_model, \n",
    "        train_loader_assembly, \n",
    "        device\n",
    "    )\n",
    "    val_acc_assembly, _ = evaluate_assembly(\n",
    "        assembly_model, \n",
    "        machinery_model, \n",
    "        val_loader_assembly, \n",
    "        device\n",
    "    )\n",
    "    scheduler.step()  # 매 에포크 후 학습률 조정\n",
    "\n",
    "    test_acc_assembly, _ = evaluate_assembly(\n",
    "        assembly_model, \n",
    "        machinery_model, \n",
    "        test_loader_assembly, \n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f\"Train Loss: {train_loss_assembly:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_acc_assembly:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_acc_assembly:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc_assembly:.4f}\")\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_acc_assembly > best_val_acc_assembly:\n",
    "        best_val_acc_assembly = val_acc_assembly\n",
    "        trigger_times = 0\n",
    "        # Best 모델 저장\n",
    "        torch.save(assembly_model.state_dict(), \"best_assembly_model.pth\")\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times: {trigger_times}\")\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "# 최종 테스트 성능 평가\n",
    "final_test_acc_assembly, final_assembly_predictions = evaluate_assembly(\n",
    "    assembly_model, \n",
    "    machinery_model, \n",
    "    test_loader_assembly, \n",
    "    device\n",
    ")\n",
    "print(f\"Final Test Accuracy (Assembly): {final_test_acc_assembly:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44071b-9711-45c5-87fa-e7b86bd3c63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999115a-fd7b-448c-939f-a12ebec64c58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ship)",
   "language": "python",
   "name": "ship"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
