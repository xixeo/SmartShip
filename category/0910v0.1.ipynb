{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83a08e1-5041-4642-99f5-c82449e37554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6526e20d-c98e-48d0-b5bb-b5f808ede823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663325ee-80a8-448e-9d0d-06d76ae99d18",
   "metadata": {},
   "source": [
    "## 모델이 클래스 특성을 학습하기에 충분한 표본 갯수로 데이터 제거\n",
    "\n",
    "> Machinery에서 데이터가 30개 이하인 클래스 수: 100\n",
    "> \n",
    "> Assembly에서 데이터가 30개 이하인 클래스 수: 1583\n",
    ">\n",
    "> 제거 후, 남은 데이터: 13882, MACHINERY : 62 ASSEMBLY:209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c7432c-d7d4-4f38-8baa-ef842df72dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('filtered_dataset_30.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64cb3b8e-7afc-406b-b683-e9fc57c549b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 209\n"
     ]
    }
   ],
   "source": [
    "print(len(data['Machinery'].unique()),len(data['Assembly'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb48e7ee-af89-4345-b401-698b61cafa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13882 entries, 0 to 13881\n",
      "Data columns (total 32 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   청구서번호        13882 non-null  object \n",
      " 1   No.          13882 non-null  int64  \n",
      " 2   Subject      13872 non-null  object \n",
      " 3   Machinery    13882 non-null  object \n",
      " 4   Assembly     13882 non-null  object \n",
      " 5   청구품목         13882 non-null  object \n",
      " 6   Unnamed: 6   0 non-null      float64\n",
      " 7   Part No.1    13881 non-null  object \n",
      " 8   Part No.2    2430 non-null   object \n",
      " 9   청구량          13818 non-null  float64\n",
      " 10  견적           13698 non-null  object \n",
      " 11  견적수량         13818 non-null  float64\n",
      " 12  견적화폐         13818 non-null  object \n",
      " 13  견적단가         13882 non-null  float64\n",
      " 14  발주번호         13882 non-null  object \n",
      " 15  발주처          13882 non-null  object \n",
      " 16  발주           13882 non-null  object \n",
      " 17  발주수량         13818 non-null  float64\n",
      " 18  발주금액         13818 non-null  float64\n",
      " 19  D/T          12461 non-null  object \n",
      " 20  미입고 기간       998 non-null    object \n",
      " 21  창고입고         11867 non-null  object \n",
      " 22  창고입고수량       13882 non-null  int64  \n",
      " 23  Control No.  8571 non-null   object \n",
      " 24  입고창고         11867 non-null  object \n",
      " 25  창고출고         10595 non-null  object \n",
      " 26  창고출고수량       13882 non-null  int64  \n",
      " 27  출고선박         10595 non-null  object \n",
      " 28  출고운반선        10595 non-null  object \n",
      " 29  선박입고         3071 non-null   object \n",
      " 30  선박입고수량       13882 non-null  int64  \n",
      " 31  완료 여부        3065 non-null   object \n",
      "dtypes: float64(6), int64(4), object(22)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e456473-b382-4547-ad15-529b584ad2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GE POWER PACK FORK - E7(B)'\n",
      " 'SAMSON SUPER STRONG DOUBLE BRAID ROPE 1 3/4\", 300FT'\n",
      " 'WIRE ROPE G)6X(S)19 A3 CMP SLPP 28MM X 400M' ... 'BRACKET '\n",
      " 'WASHER, 10 ' 'COVER,MANIFOLD.EXH ']\n"
     ]
    }
   ],
   "source": [
    "print(data['청구품목'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0aa141-ec18-48b5-9d38-39ea85bdf2bd",
   "metadata": {},
   "source": [
    "### 청구품목 전처리 \n",
    "\n",
    "1. 텍스트 전처리\n",
    "2. TF-IDF 기반 강조 (엠퍼사이징)\n",
    "3. FastText 임베딩\n",
    "   \n",
    "### part no.1 전처리\n",
    "\n",
    "> 콤마 위치에 따른 세부적인 차이가 많은 텍스트이므로 특수기호 및 문자 유지 필요 => 별도 전처리 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f68d784-6d51-43a8-8ef6-bbad5d5109d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)   \n",
    "    text = re.sub(r'[^\\w\\s\\*/\\-\\+.,#&]', '', text) \n",
    "    text = re.sub(r'\\s+', ' ', text)    \n",
    "    text = re.sub(r'\\b(사용금지|사)\\b', '', text, flags=re.IGNORECASE)    \n",
    "    text = text.strip()    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14b42583-0c68-48a8-ac07-ac3df468e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청구품목 클리닝\n",
    "data['cleaned_item'] = data['청구품목'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb665627-f3fe-4e78-ad87-29fc63c7ec43",
   "metadata": {},
   "source": [
    "> 청구품목 데이터에서는 각 단어 의미적 연관성보다 주요단어가 있는 것이므로, 가중치 부여하는 것으로 접근함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da4cb6a1-43d8-40ce-ade2-57c1bd046dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as' 'bearing' 'bolt' 'charges' 'core' 'cover' 'cylinder' 'for' 'fuel'\n",
      " 'gasket' 'gear' 'gp' 'head' 'hex' 'in' 'kit' 'nut' 'oil' 'plate' 'pump'\n",
      " 'ring' 'screw' 'seal' 'sensor' 'set' 'shaft' 'spring' 'valve' 'washer'\n",
      " 'water']\n"
     ]
    }
   ],
   "source": [
    "# 확인용\n",
    "claim_items = data['청구품목'].tolist() \n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer(max_features=30)\n",
    "tfidf_matrix = tfidf.fit_transform(claim_items)\n",
    "\n",
    "# 중요한 단어 추출\n",
    "important_words = tfidf.get_feature_names_out()\n",
    "\n",
    "print(important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "978a32ad-6eab-40b1-b6e6-9f0072d24d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 2. TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer(max_features=30) \n",
    "tfidf_matrix = tfidf.fit_transform(data['cleaned_item'])\n",
    "\n",
    "important_words = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f90ec96-02f3-4634-90af-2d1ca1b87d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "# NLTK 증강라이브러리 다운로드\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# 한번에 4개씩 동의어 교체, 새로운 단어 무작위 삽입\n",
    "\n",
    "\n",
    "def synonym_replacement(text, n=2):\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replacements = min(n, len(random_word_list))\n",
    "    for word in random_word_list[:num_replacements]:\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "        new_words = [synonym if word == w else w for w in new_words]\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def random_insertion(text, n=2):\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        synonym_word = random.choice(words)\n",
    "        synonyms = wordnet.synsets(synonym_word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms).lemmas()[0].name()\n",
    "            insert_position = random.randint(0, len(new_words))\n",
    "            new_words.insert(insert_position, synonym)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# 데이터 증강 함수 통과 후 -> 변형된 텍스트\n",
    "def augment_text(text):\n",
    "    augmented_text = synonym_replacement(text, n=4)\n",
    "    augmented_text = random_insertion(augmented_text, n=4)\n",
    "    return augmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6eecb2aa-54d8-4ea1-8b88-056dcd9b16af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['augmented_item'] = data['emphasized_item'].apply(augment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590b98d-a4ec-4fa7-b9a3-a885f465b885",
   "metadata": {},
   "source": [
    "### 발주처 전처리 강화\n",
    "\n",
    "> 부가단어 (CORPORATION, Corp, CO., Ltd, GmbH, Co., Inc, 주식회사, 상사, 공사, Co.,Ltd, Ltd, Pte Ltd, LLC) 제거\n",
    "\n",
    "> 핵심 정보(회사명 직접 관련) emphasizing 함 > 증강함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a047f01f-acfa-4bf9-b772-c35b47e279e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_supplier_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r'coporation|coropration|coproration|corporration', 'corporation', name)\n",
    "    name = re.sub(r'\\(사용금지\\)', '', name)\n",
    "    name = re.sub(r'u\\.s\\.a', '_usa', name)\n",
    "    name = re.sub(r'\\.', '', name)\n",
    "    suffixes = r'(corporation|corp|company|co|incorporated|inc|limited|ltd|상사|공사|엔지니어링|주식회사|주|gmbh|pte ltd|llc)'\n",
    "    name = re.sub(suffixes, '', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93715f68-009b-424f-b42b-1e928ec887ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matsui_usa', 'kti', '대광기업', 'kth mar', 'haein _cheonan', 'korea ucd', 'east wind', '인스알파', 'in international', '한국쉘석유', 'euro kytex engineering bv', '대동베아링', 'marine hydrotec', '금안', 'test', 'port relief engineering', 'caterpillar marine asia pacific', '혜인', 'sanwa mmercial', 'yusinhr', '선진종합', 'furuno', 'nissin refrigeration engineerin', '우림', 'haein _cheonan', 'kemel', 'rexnord -falk marine group', '유신에이치알', 'gea korea', '안에너지', 'sunjin etech', '디에스알제강', '한국에프에이디', 'albert', 'wartsila korea', '선진엔텍', 'piriou naval', '프러스', 'taeyoung enterprise', 'shina', 'ins alfa', 'kemelkomarine', '누리', 'rnk tech', 'os system', '씨코리아', '두원알앤에이', '합동듸젤사', '하이에어코리아', 'desmi pumping technologysuzhou', '한국마이콤', 'human engineering']\n"
     ]
    }
   ],
   "source": [
    "suppliers = [\n",
    "    'MATSUI(U.S.A) COROPRATION', 'KTI', '대광기업(주)', 'K.TH MARCO',\n",
    "    'HAEIN Coporation_Cheonan', 'KOREA UCD CO.,LTD.', 'EAST WIND Gmbh', '인스알파',\n",
    "    'ICON INTERNATIONAL, INC', '한국쉘석유㈜', 'EURO KYTEX ENGINEERING BV', '대동베아링상사',\n",
    "    'MARINE HYDROTEC CO.,LTD.', '금안상사', 'TEST COMPANY',\n",
    "    'PORT RELIEF ENGINEERING CO.,LTD.',\n",
    "    'Caterpillar Marine Asia Pacific Pte Ltd', '(주)혜인',\n",
    "    'SANWA COMMERCIAL CO.,LTD.', 'yusinHR Co., Ltd.', '(주)선진종합', 'FURUNO',\n",
    "    'NISSIN REFRIGERATION  ENGINEERIN', '(주)우림공사',\n",
    "    'HAEIN Coporation_Cheonan(사용금지)', 'KEMEL', 'REXNORD LLC-FALK MARINE GROUP',\n",
    "    '유신에이치알(사용금지)', 'GEA KOREA LTD', '주안에너지㈜', 'SUNJIN ETECH Co.,Ltd.',\n",
    "    '디에스알제강주식회사', '(주)한국에프에이디', 'ALBERT GMBH', 'Wartsila Korea Ltd.',\n",
    "    '(주)선진엔텍(사용금지)', 'PIRIOU NAVAL', '(주)프러스엔지니어링', 'Taeyoung Enterprise',\n",
    "    'SHINA', 'INS ALFA', 'KEMEL(KOMARINE)', '누리엔지니어링', 'RNK TECH CO.,LTD',\n",
    "    'OS SYSTEM CO.,LTD', '씨코리아엔지니어링(주)', '(주)두원알앤에이', '합동듸젤사', '하이에어코리아(주)',\n",
    "    'DESMI PUMPING TECHNOLOGY(SUZHOU) CO.,LTD', '한국마이콤',\n",
    "    'HUMAN & ENGINEERING CO.,LTD'\n",
    "]\n",
    "\n",
    "cleaned_suppliers = [clean_supplier_name(supplier) for supplier in suppliers]\n",
    "print(cleaned_suppliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "668b9875-82a8-43c5-bb4f-c1b6bd535e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  HAEIN Corporation => HAEIN\n",
    "def extract_important_part(name):\n",
    "    if re.search(r'[가-힣]', name):\n",
    "        name = re.sub(r'(기업|상사|종합|공사)', '', name)\n",
    "        important_part = name.split()[0]  # 첫 단어 추출\n",
    "    else:\n",
    "        # 영문 이름의 경우 첫 번째 단어만 추출\n",
    "        important_part = name.split()[0]\n",
    "    \n",
    "    return important_part\n",
    "\n",
    "# 한번더 반복 HAEIN HAEIN Corporation\n",
    "def emphasize_supplier_name(name):\n",
    "    important_part = extract_important_part(name)\n",
    "    emphasized_name = f\"{important_part} {important_part} {name}\"  # 중요한 부분을 반복\n",
    "    return emphasized_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ece38a1f-a83e-4021-afb4-057883d6e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matsui_usa matsui_usa matsui_usa', 'taeyoung taeyoung taeyoung enterprise', 'haein haein haein _cheonan']\n"
     ]
    }
   ],
   "source": [
    "suppliers = ['MATSUI(U.S.A) COROPRATION', 'taeyoung enterprise','HAEIN Coporation_Cheonan(사용금지)']\n",
    "cleaned_suppliers = [clean_supplier_name(supplier) for supplier in suppliers]  # 전처리\n",
    "emphasized_suppliers = [emphasize_supplier_name(supplier) for supplier in cleaned_suppliers]  # 강조\n",
    "\n",
    "print(emphasized_suppliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8078a0c-6a95-46d3-bb74-ac2798e556fd",
   "metadata": {},
   "source": [
    "### 결합\n",
    "\n",
    "> 청구품목 데이터 증강 수행 후, 원본 발주처 및 part no.1과 일대일로 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1977ab6f-cdc6-4bef-a91f-2bbb62b77a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청구품목 = TF-IDF 강조 적용 + 증강된 것\n",
    "# data['augmented_item'] = data['emphasized_item'].apply(augment_text)\n",
    "\n",
    "#파트넘버(별도 전처리X)\n",
    "data['Part No.1'] = data['Part No.1'].astype(str)\n",
    "\n",
    "# 발주처\n",
    "data['cleaned_supplier'] = data['발주처'].apply(clean_supplier_name)\n",
    "data['emphasized_supplier'] = data['cleaned_supplier'].apply(emphasize_supplier_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9aad7f10-d55b-47c1-b2a6-bd4fe5c5ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmented_data = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    # 청구품목을 10개씩 증강하고 그 수만큼 발주처와 Part No.1을 복제\n",
    "    augmented_item_list = [augment_text(row['augmented_item']) for _ in range(10)]\n",
    "    for augmented_item in augmented_item_list:\n",
    "        augmented_data.append([augmented_item, row['Part No.1'], row['emphasized_supplier']])\n",
    "\n",
    "# 증강된 데이터를 DataFrame으로 변환\n",
    "augmented_df = pd.DataFrame(augmented_data, columns=['augmented_item', 'Part No.1', 'emphasized_supplier'])\n",
    "\n",
    "# 원본 데이터와 결합 (원본 데이터도 포함)\n",
    "combined_df = pd.concat([data[['augmented_item', 'Part No.1', 'emphasized_supplier']], augmented_df], ignore_index=True)\n",
    "\n",
    "# 최종 combined_text 결합\n",
    "combined_df['combined_text'] = combined_df['augmented_item'].fillna('') + \" \" + combined_df['Part No.1'].fillna('') + \" \" + combined_df['emphasized_supplier'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ad01688-8522-4ee4-9221-5fe32a61b9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>augmented_item</th>\n",
       "      <th>Part No.1</th>\n",
       "      <th>emphasized_supplier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gaea Gaea might Gaea might Gaea pack fork - e7...</td>\n",
       "      <td>40028340</td>\n",
       "      <td>matsui_usa matsui_usa matsui_usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gaea power Gaea power pack Gaea fork - exponen...</td>\n",
       "      <td>40028340</td>\n",
       "      <td>matsui_usa matsui_usa matsui_usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gaea power Gaea power battalion pitchfork - e7...</td>\n",
       "      <td>40028340</td>\n",
       "      <td>matsui_usa matsui_usa matsui_usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gaea world_power Gaea world_power tamp_down Ga...</td>\n",
       "      <td>40028340</td>\n",
       "      <td>matsui_usa matsui_usa matsui_usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gaea Gaea Gaea might Gaea might pack fork - e7...</td>\n",
       "      <td>40028340</td>\n",
       "      <td>matsui_usa matsui_usa matsui_usa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      augmented_item Part No.1  \\\n",
       "0  Gaea Gaea might Gaea might Gaea pack fork - e7...  40028340   \n",
       "1  Gaea power Gaea power pack Gaea fork - exponen...  40028340   \n",
       "2  Gaea power Gaea power battalion pitchfork - e7...  40028340   \n",
       "3  Gaea world_power Gaea world_power tamp_down Ga...  40028340   \n",
       "4  Gaea Gaea Gaea might Gaea might pack fork - e7...  40028340   \n",
       "\n",
       "                emphasized_supplier  \n",
       "0  matsui_usa matsui_usa matsui_usa  \n",
       "1  matsui_usa matsui_usa matsui_usa  \n",
       "2  matsui_usa matsui_usa matsui_usa  \n",
       "3  matsui_usa matsui_usa matsui_usa  \n",
       "4  matsui_usa matsui_usa matsui_usa  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_df = pd.DataFrame(augmented_data, columns=['augmented_item', 'Part No.1', 'emphasized_supplier'])\n",
    "augmented_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53051fb1-ded0-4596-9d2b-fa069de0610e",
   "metadata": {},
   "source": [
    "## fasttext 임베딩\n",
    "> 전처리에서 가중치 부여된 데이터로 fasttext 학습했으므로 임베딩에서는 가중처리 방식 선택하지 않음\n",
    "\n",
    "> 평균 시퀀스를 기준 패딩처리함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867bc16f-a891-4527-9db4-392aa3569d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3371424, 5151930)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [text.split() for text in data['combined_text'].tolist()]\n",
    "\n",
    "# FastText 모델, 사전, 트레인\n",
    "ft_model = FastText(vector_size=120, window=5, min_count=1, min_n=2, max_n=7, sg=1)\n",
    "ft_model.build_vocab(sentences)\n",
    "ft_model.train(sentences, total_examples=len(sentences), epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fcf9040-cac0-4208-a9bf-cc2dad99eb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 시퀀스 길이: 12.370767900878835\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = [len(text.split()) for text in data['combined_text']]\n",
    "average_length = sum(sequence_lengths) / len(sequence_lengths)\n",
    "print(f\"평균 시퀀스 길이: {average_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d33bad-1525-41c1-aa64-b38104c4c675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13576\\2339533233.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  item_embedding = torch.tensor([ft_model.wv[word] for word in row['augmented_item'].split() if word in ft_model.wv])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13882, 48, 120])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 패딩 함수\n",
    "def pad_embeddings(embeddings, target_length, model):\n",
    "    current_length = embeddings.size(0)\n",
    "    if current_length < target_length:\n",
    "        padding = torch.zeros(target_length - current_length, model.vector_size)\n",
    "        embeddings = torch.cat([embeddings, padding], dim=0)\n",
    "    elif current_length > target_length:\n",
    "        embeddings = embeddings[:target_length]\n",
    "    return embeddings\n",
    "\n",
    "# 패딩처리하고 결합, 시퀀스 길이에 맞춰 타겟 길이 조정함\n",
    "target_length = 16\n",
    "combined_embeddings = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    # **증강된 청구품목**을 반영하여 임베딩 생성\n",
    "    item_embedding = torch.tensor([ft_model.wv[word] for word in row['augmented_item'].split() if word in ft_model.wv])\n",
    "    part_embedding = torch.tensor([ft_model.wv[word] for word in row['Part No.1'].split() if word in ft_model.wv])  # 전처리X\n",
    "    supplier_embedding = torch.tensor([ft_model.wv[word] for word in row['emphasized_supplier'].split() if word in ft_model.wv])\n",
    "\n",
    "    # 각각의 임베딩을 패딩하여 고정 크기로 맞춤\n",
    "    item_embedding_padded = pad_embeddings(item_embedding, target_length, ft_model)\n",
    "    part_embedding_padded = pad_embeddings(part_embedding, target_length, ft_model)\n",
    "    supplier_embedding_padded = pad_embeddings(supplier_embedding, target_length, ft_model)\n",
    "\n",
    "    # 패딩된 임베딩들을 결합\n",
    "    combined_embedding = torch.cat([item_embedding_padded, part_embedding_padded, supplier_embedding_padded], dim=0)\n",
    "    combined_embeddings.append(combined_embedding)\n",
    "\n",
    "# 최종 텐서로 변환\n",
    "combined_embeddings_tensor = torch.stack(combined_embeddings)\n",
    "\n",
    "X = combined_embeddings_tensor\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf009e4d-8dd6-4e58-a3ad-df4efd1bdd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# 정수형 레이블이어야 분류할 수 있음\n",
    "machinery_labels = data['Machinery'].values\n",
    "assembly_labels = data['Assembly'].values\n",
    "\n",
    "label_encoder_machinery = LabelEncoder()\n",
    "y_machinery = label_encoder_machinery.fit_transform(machinery_labels)\n",
    "\n",
    "label_encoder_assembly = LabelEncoder()\n",
    "y_assembly = label_encoder_assembly.fit_transform(assembly_labels)\n",
    "\n",
    "# 스케일러 적용\n",
    "X = combined_embeddings_tensor\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "\n",
    "# Train-Test Split 그대로 유지\n",
    "X_train_val, X_test, y_train_val_machinery, y_test_machinery, y_train_val_assembly, y_test_assembly = train_test_split(\n",
    "    X_scaled, y_machinery, y_assembly, test_size=0.2, random_state=42, stratify=y_machinery\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train_machinery, y_val_machinery, y_train_assembly, y_val_assembly = train_test_split(\n",
    "    X_train_val, y_train_val_machinery, y_train_val_assembly, test_size=0.2, random_state=42, stratify=y_train_val_machinery\n",
    ")\n",
    "\n",
    "# CNN 모델용 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_machinery, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val_machinery, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_machinery, dtype=torch.long)\n",
    "\n",
    "# XGBoost 모델용 NumPy 배열 준비 (이미 X_scaled는 NumPy 배열)\n",
    "X_train_np = X_train\n",
    "X_val_np = X_val\n",
    "X_test_np = X_test\n",
    "\n",
    "y_train_np = y_train_machinery\n",
    "y_val_np = y_val_machinery\n",
    "y_test_np = y_test_machinery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888cb8b0-7a6c-4f48-9792-dc44cbe61a9e",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eee6727-d855-41bb-be38-1fe540793f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.2757 - loss: 5.0515 - val_accuracy: 0.0662 - val_loss: 5.1046 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5335 - loss: 3.7729 - val_accuracy: 0.5700 - val_loss: 3.3911 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5605 - loss: 3.3634 - val_accuracy: 0.6119 - val_loss: 2.9379 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5788 - loss: 3.1024 - val_accuracy: 0.6177 - val_loss: 2.8218 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.5937 - loss: 2.8984 - val_accuracy: 0.6326 - val_loss: 2.6309 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6164 - loss: 2.7218 - val_accuracy: 0.6236 - val_loss: 2.5297 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6220 - loss: 2.5466 - val_accuracy: 0.6628 - val_loss: 2.2781 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6521 - loss: 2.3639 - val_accuracy: 0.6457 - val_loss: 2.2362 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6540 - loss: 2.2428 - val_accuracy: 0.6767 - val_loss: 2.1193 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6687 - loss: 2.0989 - val_accuracy: 0.6844 - val_loss: 1.9748 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6953 - loss: 1.9663 - val_accuracy: 0.7114 - val_loss: 1.8812 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.6959 - loss: 1.8736 - val_accuracy: 0.7051 - val_loss: 1.8096 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7021 - loss: 1.7970 - val_accuracy: 0.7064 - val_loss: 1.7633 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7143 - loss: 1.7083 - val_accuracy: 0.7015 - val_loss: 1.7206 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7270 - loss: 1.6111 - val_accuracy: 0.7136 - val_loss: 1.6174 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7444 - loss: 1.5198 - val_accuracy: 0.7001 - val_loss: 1.6518 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7421 - loss: 1.4701 - val_accuracy: 0.7253 - val_loss: 1.5297 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7546 - loss: 1.3941 - val_accuracy: 0.7010 - val_loss: 1.5621 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7699 - loss: 1.3217 - val_accuracy: 0.7294 - val_loss: 1.4449 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7721 - loss: 1.2586 - val_accuracy: 0.7380 - val_loss: 1.4188 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7846 - loss: 1.2084 - val_accuracy: 0.7006 - val_loss: 1.4258 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7834 - loss: 1.1603 - val_accuracy: 0.7447 - val_loss: 1.3816 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8017 - loss: 1.1005 - val_accuracy: 0.6920 - val_loss: 1.4574 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8005 - loss: 1.0651 - val_accuracy: 0.7407 - val_loss: 1.3066 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8190 - loss: 1.0046 - val_accuracy: 0.7438 - val_loss: 1.2429 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8268 - loss: 0.9461 - val_accuracy: 0.6569 - val_loss: 1.4316 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8377 - loss: 0.8999 - val_accuracy: 0.7294 - val_loss: 1.4179 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8338 - loss: 0.8913 - val_accuracy: 0.7073 - val_loss: 1.4361 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8520 - loss: 0.8298 - val_accuracy: 0.7411 - val_loss: 1.2548 - learning_rate: 5.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8625 - loss: 0.7784 - val_accuracy: 0.7443 - val_loss: 1.2316 - learning_rate: 5.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8736 - loss: 0.7564 - val_accuracy: 0.7461 - val_loss: 1.2389 - learning_rate: 5.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8692 - loss: 0.7518 - val_accuracy: 0.7492 - val_loss: 1.2243 - learning_rate: 5.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8858 - loss: 0.7058 - val_accuracy: 0.7519 - val_loss: 1.2931 - learning_rate: 5.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8839 - loss: 0.6943 - val_accuracy: 0.7447 - val_loss: 1.2427 - learning_rate: 5.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8830 - loss: 0.6903 - val_accuracy: 0.7353 - val_loss: 1.2167 - learning_rate: 5.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8983 - loss: 0.6511 - val_accuracy: 0.7492 - val_loss: 1.2602 - learning_rate: 5.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9021 - loss: 0.6422 - val_accuracy: 0.7416 - val_loss: 1.1972 - learning_rate: 5.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9081 - loss: 0.6157 - val_accuracy: 0.7231 - val_loss: 1.2046 - learning_rate: 5.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9060 - loss: 0.6130 - val_accuracy: 0.7384 - val_loss: 1.2164 - learning_rate: 5.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9034 - loss: 0.6045 - val_accuracy: 0.7281 - val_loss: 1.2300 - learning_rate: 5.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9107 - loss: 0.5913 - val_accuracy: 0.7591 - val_loss: 1.2360 - learning_rate: 2.5000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9194 - loss: 0.5623 - val_accuracy: 0.7605 - val_loss: 1.2672 - learning_rate: 2.5000e-05\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, GlobalMaxPooling1D, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# 개선된 CNN 모델\n",
    "machinery_model = Sequential([\n",
    "    Conv1D(256, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Conv1D(256, kernel_size=5, activation='relu'),  # 추가 Conv1D 레이어\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    GlobalMaxPooling1D(),  \n",
    "    \n",
    "    Dense(128, kernel_regularizer=l2(0.01)), \n",
    "    LeakyReLU(alpha=0.01),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),  # Dropout 비율 조정\n",
    "    \n",
    "    Dense(64, kernel_regularizer=l2(0.001)),  \n",
    "    LeakyReLU(alpha=0.01),\n",
    "    Dropout(0.3),  # Dropout 추가\n",
    "    \n",
    "    Dense(62, activation='softmax')  # Machinery 클래스 수\n",
    "])\n",
    "\n",
    "# 옵티마이저와 학습률 조정\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "machinery_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 콜백 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "\n",
    "# 모델 학습\n",
    "history = machinery_model.fit(X_train, y_train_machinery, epochs=50, batch_size=32, validation_data=(X_val, y_val_machinery), callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ec2a8be-0e65-47b7-91f8-008b485882df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "machinery_optimized_model = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=62,\n",
    "    learning_rate=0.0746,\n",
    "    max_depth=10,\n",
    "    n_estimators=148,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.6804,\n",
    "    reg_lambda=1,\n",
    "    random_state=42,\n",
    "    verbosity=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7952a2e3-9e01-4422-bcc0-40b81c5407a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 34 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please reshape the input data into 2-dimensional matrix.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 55\u001b[0m\n\u001b[0;32m     46\u001b[0m stacked_model \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[0;32m     47\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     48\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m'\u001b[39m, Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn_transformer\u001b[39m\u001b[38;5;124m'\u001b[39m, cnn_transformer)])),  \u001b[38;5;66;03m# CNN 모델 변환 후 입력\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     final_estimator\u001b[38;5;241m=\u001b[39mLogisticRegression()  \u001b[38;5;66;03m# 최종 분류기로 Logistic Regression 사용\u001b[39;00m\n\u001b[0;32m     52\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 3. Stacking 모델 학습\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43mstacked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_machinery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 예측 및 평가\u001b[39;00m\n\u001b[0;32m     58\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m stacked_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:672\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    671\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:224\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_estimators\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m    231\u001b[0m est_fitted_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:40\u001b[0m, in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, fit_params, message_clsname, message)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m---> 40\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\sklearn.py:1512\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m-> 1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1532\u001b[0m     params,\n\u001b[0;32m   1533\u001b[0m     train_dmatrix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1542\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1543\u001b[0m )\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\sklearn.py:596\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[0;32m    577\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m    578\u001b[0m     X: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    592\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 596\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\sklearn.py:1003\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1003\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\core.py:1573\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1554\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m         )\n\u001b[0;32m   1567\u001b[0m     ):\n\u001b[0;32m   1568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1571\u001b[0m         )\n\u001b[1;32m-> 1573\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\core.py:1632\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[1;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[0;32m   1620\u001b[0m config \u001b[38;5;241m=\u001b[39m make_jcargs(\n\u001b[0;32m   1621\u001b[0m     nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthread, missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[0;32m   1622\u001b[0m )\n\u001b[0;32m   1623\u001b[0m ret \u001b[38;5;241m=\u001b[39m _LIB\u001b[38;5;241m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[0;32m   1624\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1625\u001b[0m     it\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1630\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(handle),\n\u001b[0;32m   1631\u001b[0m )\n\u001b[1;32m-> 1632\u001b[0m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m _check_call(ret)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\core.py:569\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    567\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\core.py:550\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[1;34m(self, fn, dft_ret)\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[0;32m    555\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\core.py:637\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\data.py:1416\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1416\u001b[0m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\core.py:625\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[1;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n\u001b[1;32m--> 625\u001b[0m \u001b[43mdispatch_proxy_set_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_codes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mset_info(\n\u001b[0;32m    627\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m    628\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    630\u001b[0m )\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_ref \u001b[38;5;241m=\u001b[39m ref\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\data.py:1473\u001b[0m, in \u001b[0;36mdispatch_proxy_set_data\u001b[1;34m(proxy, data, cat_codes)\u001b[0m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch for QuantileDMatrix.\"\"\"\u001b[39;00m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_cudf_ser(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pandas_series(data):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[43m_check_data_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_cudf_df(data):\n\u001b[0;32m   1476\u001b[0m     \u001b[38;5;66;03m# pylint: disable=W0212\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m     proxy\u001b[38;5;241m.\u001b[39m_ref_data_from_cuda_columnar(data, cast(List, cat_codes))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ship\\Lib\\site-packages\\xgboost\\data.py:58\u001b[0m, in \u001b[0;36m_check_data_shape\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_data_shape\u001b[39m(data: DataType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 58\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease reshape the input data into 2-dimensional matrix.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Please reshape the input data into 2-dimensional matrix."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import TransformerMixin\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, GlobalMaxPooling1D, LeakyReLU\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. CNN 모델 정의 (변환기 클래스 사용)\n",
    "class CNNTransformer(TransformerMixin):\n",
    "    def __init__(self, input_shape, epochs=100, batch_size=32):\n",
    "        self.model = self.build_cnn_model(input_shape)\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def build_cnn_model(self, input_shape):\n",
    "        model = Sequential([\n",
    "            Conv1D(256, kernel_size=5, activation='relu', input_shape=input_shape),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            BatchNormalization(),\n",
    "            Conv1D(256, kernel_size=5, activation='relu'),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            BatchNormalization(),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(128, kernel_regularizer=l2(0.01)),\n",
    "            LeakyReLU(alpha=0.01),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.4),\n",
    "            Dense(64, kernel_regularizer=l2(0.001)),\n",
    "            LeakyReLU(alpha=0.01),\n",
    "            Dropout(0.3),\n",
    "            Dense(62, activation='softmax')  # Machinery 클래스 수\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# 2. Stacking 모델 정의\n",
    "cnn_transformer = CNNTransformer(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('cnn', Pipeline([('cnn_transformer', cnn_transformer)])),  # CNN 모델 변환 후 입력\n",
    "        ('xgb', machinery_optimized_model)  # XGBoost 모델\n",
    "    ],\n",
    "    final_estimator=LogisticRegression()  # 최종 분류기로 Logistic Regression 사용\n",
    ")\n",
    "\n",
    "# 3. Stacking 모델 학습\n",
    "stacked_model.fit(X_train, y_train_machinery)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = stacked_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_machinery, y_pred)\n",
    "print(f\"Stacked Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfc072-09a3-4c80-966b-83d81d18a115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ship)",
   "language": "python",
   "name": "ship"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
