{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de827958-8584-401f-9d62-1c3ecb4de85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9715e073-3876-4b8a-8d27-9987e232a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('filtered_30_filled_money.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94099074-a4c5-4800-877c-2d657cd73fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    text = re.sub(r'[^\\w\\s\\*/\\-\\+.,#&]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\b(사용금지|사)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_supplier_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r'coporation|coropration|coproration|corporration', 'corporation', name)\n",
    "    name = re.sub(r'\\(사용금지\\)', '', name)\n",
    "    name = re.sub(r'u\\.s\\.a', '_usa', name)\n",
    "    name = re.sub(r'\\.', '', name)\n",
    "    suffixes = r'(corporation|corp|company|co|incorporated|inc|limited|ltd|상사|공사|엔지니어링|주식회사|주|gmbh|pte ltd|llc)'\n",
    "    name = re.sub(suffixes, '', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bd270e-abd8-44ab-a486-48469d26853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리\n",
    "data['cleaned_item'] = data['청구품목'].apply(preprocess_text)\n",
    "data['cleaned_supplier'] = data['발주처'].apply(clean_supplier_name)\n",
    "data['combined_text'] = data['cleaned_item'].fillna('') + \" \" + data['Part No.1'].fillna('') + \" \" + data['cleaned_supplier'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f5153d-187c-494f-bc74-1194a8f7c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_rates = {'USD': 1, 'KRW': 0.00078, 'EUR': 1.18, 'JPY': 0.0091}\n",
    "\n",
    "# usd기준해서 금액 통일함 \n",
    "data['converted_price'] = data.apply(lambda x: x['견적단가'] * exchange_rates[x['견적화폐']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b45129-3e52-490c-a953-21ef3b1756d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# '견적화폐' 컬럼을 OneHotEncoder를 통해 인코딩\n",
    "currency_ohe = OneHotEncoder(sparse_output=False) \n",
    "currency_encoded = currency_ohe.fit_transform(data[['견적화폐']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "468f76da-0af7-455f-86ba-f4ccd69b12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data['converted_price_log'] = np.log1p(data['converted_price'])  # 로그 변환된 가격\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4655b8ab-ae1b-4280-b44b-1d0d2ae15695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 인코딩\n",
    "machinery_label_encoder = LabelEncoder()\n",
    "y_machinery= machinery_label_encoder.fit_transform(data['Machinery'])\n",
    "\n",
    "assembly_label_encoder = LabelEncoder()\n",
    "y_assembly = assembly_label_encoder.fit_transform(data['Assembly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8522452-bdfc-4c8a-9027-d460b64428bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_text shape: (13882,)\n",
      "currency_encoded shape: (13882, 4)\n",
      "converted_price shape: (13882,)\n",
      "X shape after concatenation: (13882, 6)\n",
      "X_train size: (10029, 6)\n",
      "X_val size: (1770, 6)\n",
      "X_test size: (2083, 6)\n"
     ]
    }
   ],
   "source": [
    "# train_test split 을 위해 하나로 모으고, 분할하고 다시 텍스트랑 추가피쳐로 분리해줄거임 \n",
    "\n",
    "X = np.concatenate([\n",
    "    data['combined_text'].values.reshape(-1, 1),  # 2차원 배열로 바꿔서 결합해줌 \n",
    "    currency_encoded.astype(float),  # currency_encoded를 float로 변환\n",
    "    data['converted_price_log'].values.reshape(-1, 1)  # 통일한단가\n",
    "], axis=1)\n",
    "\n",
    "X_train_val, X_test, y_train_val_machinery, y_test_machinery, y_train_val_assembly, y_test_assembly = train_test_split(\n",
    "    X, y_machinery, y_assembly, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=y_assembly)  # stratify는 주로 메인 레이블 기준으로 설정\n",
    "\n",
    "X_train, X_val, y_train_machinery, y_val_machinery, y_train_assembly, y_val_assembly = train_test_split(\n",
    "    X_train_val, y_train_val_machinery, y_train_val_assembly, \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    stratify=y_train_val_assembly)  # 다시 stratify 기준으로 설정\n",
    "\n",
    "# 크기 확인\n",
    "print(f\"combined_text shape: {data['combined_text'].shape}\")\n",
    "print(f\"currency_encoded shape: {currency_encoded.shape}\")\n",
    "print(f\"converted_price shape: {data['converted_price'].shape}\")\n",
    "print(f\"X shape after concatenation: {X.shape}\")\n",
    "\n",
    "print(f\"X_train size: {X_train.shape}\")\n",
    "print(f\"X_val size: {X_val.shape}\")\n",
    "print(f\"X_test size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1205a554-0603-4f20-b3b1-bbf075614ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분리\n",
    "train_combined_text = X_train[:, 0]\n",
    "val_combined_text = X_val[:, 0]\n",
    "test_combined_text = X_test[:, 0]\n",
    "\n",
    "# 추가 피처 분리 (currency_encoded와 로그 변환된 가격)\n",
    "train_extra_features = X_train[:, 1:]\n",
    "val_extra_features = X_val[:, 1:]\n",
    "test_extra_features = X_test[:, 1:]\n",
    "\n",
    "# 추가 피처 분리 (원핫 인코딩된 통화와 로그 변환된 가격)\n",
    "train_currency_encoded = train_extra_features[:, :-1]  # 마지막 컬럼 제외 (원핫 인코딩된 통화)\n",
    "val_currency_encoded = val_extra_features[:, :-1]\n",
    "test_currency_encoded = test_extra_features[:, :-1]\n",
    "\n",
    "train_price_log = train_extra_features[:, -1].reshape(-1, 1)  # 마지막 컬럼만 (로그 변환된 가격)\n",
    "val_price_log = val_extra_features[:, -1].reshape(-1, 1)\n",
    "test_price_log = test_extra_features[:, -1].reshape(-1, 1)\n",
    "\n",
    "train_currency_encoded = train_currency_encoded.astype(float)\n",
    "val_currency_encoded = val_currency_encoded.astype(float)\n",
    "test_currency_encoded = test_currency_encoded.astype(float)\n",
    "\n",
    "train_price_log = train_price_log.astype(float)\n",
    "val_price_log = val_price_log.astype(float)\n",
    "test_price_log = test_price_log.astype(float)\n",
    "\n",
    "# 로그 변환된 가격에만 스케일링 적용\n",
    "scaler = StandardScaler()\n",
    "train_price_log_scaled = scaler.fit_transform(train_price_log)\n",
    "val_price_log_scaled = scaler.transform(val_price_log)\n",
    "test_price_log_scaled = scaler.transform(test_price_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b16b34f-752a-4584-858f-0ada60760d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currency_encoded dtype: float64\n",
      "X dtype after concatenation: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"currency_encoded dtype: {currency_encoded.dtype}\")\n",
    "print(f\"X dtype after concatenation: {X.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5791661-7107-4576-b60d-69e367c4b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 피처 결합 (원핫 인코딩된 통화 + 스케일링된 가격)\n",
    "train_final_features = np.hstack([train_currency_encoded, train_price_log])\n",
    "val_final_features = np.hstack([val_currency_encoded, val_price_log])\n",
    "test_final_features = np.hstack([test_currency_encoded, test_price_log])\n",
    "\n",
    "# float 변환 (결합 후 타입 변환)\n",
    "train_final_features = train_final_features.astype(float)\n",
    "val_final_features = val_final_features.astype(float)\n",
    "test_final_features = test_final_features.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e1270e8-9b60-4454-9851-b0c1d80e44fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14760\\1832605480.py:3: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  train_extra_features_tensor = torch.tensor(train_final_features, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Torch Tensor로 변환\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_extra_features_tensor = torch.tensor(train_final_features, dtype=torch.float32).to(device)\n",
    "val_extra_features_tensor = torch.tensor(val_final_features, dtype=torch.float32).to(device)\n",
    "test_extra_features_tensor = torch.tensor(test_final_features, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12affe13-a536-4714-913c-70912dd1abba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이저 (텍스트처리)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44558653-ff97-42cb-a192-c6b0dd9e8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X중 텍스트만 BERT 입력 형식으로 변환\n",
    "def encode_data(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "train_machinery_encodings = encode_data(train_combined_text)\n",
    "val_machinery_encodings = encode_data(val_combined_text)\n",
    "test_machinery_encodings = encode_data(test_combined_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65e97158-70b4-4ece-99f5-8bf3c78f476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_machinery_dataset = TensorDataset(\n",
    "    train_machinery_encodings['input_ids'],\n",
    "    train_machinery_encodings['attention_mask'],\n",
    "    train_extra_features_tensor,\n",
    "    torch.tensor(y_train_machinery, dtype=torch.long).to(device)  # Machinery 레이블\n",
    ")\n",
    "\n",
    "val_machinery_dataset = TensorDataset(\n",
    "    val_machinery_encodings['input_ids'],\n",
    "    val_machinery_encodings['attention_mask'],\n",
    "    val_extra_features_tensor,\n",
    "    torch.tensor(y_val_machinery, dtype=torch.long).to(device)\n",
    ")\n",
    "\n",
    "test_machinery_dataset = TensorDataset(\n",
    "    test_machinery_encodings['input_ids'],\n",
    "    test_machinery_encodings['attention_mask'],\n",
    "    test_extra_features_tensor,\n",
    "    torch.tensor(y_test_machinery, dtype=torch.long).to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c6fc79-7f45-4a76-b7ca-001707c64f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train size: (10029,)\n",
      "y_val size: (1770,)\n",
      "y_test size: (2083,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train size: {y_train_machinery.shape}\")\n",
    "print(f\"y_val size: {y_val_machinery.shape}\")\n",
    "print(f\"y_test size: {y_test_machinery.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79c5b89d-d987-4d05-9d72-e1ba20c3e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DataLoader 생성\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader_machinery = DataLoader(train_machinery_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader_machinery  = DataLoader(val_machinery_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader_machinery = DataLoader(test_machinery_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38a8452c-0350-448f-b5c5-641b8e512ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMachinery(nn.Module):\n",
    "    def __init__(self, num_machinery_labels, extra_features_dim):\n",
    "        super(BertForMachinery, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc1 = nn.Linear(773, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.machinery_classifier = nn.Linear(256, num_machinery_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, extra_features):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        if extra_features.dim() == 1:\n",
    "            extra_features = extra_features.unsqueeze(1)\n",
    "        \n",
    "        machinery_combined_features = torch.cat((pooled_output, extra_features), dim=1)\n",
    "        x = self.fc1(machinery_combined_features)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        machinery_outputs = self.machinery_classifier(x)\n",
    "        \n",
    "        return machinery_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3f8589d-d1e7-4fcd-9d91-0634e8e6249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62779f71-0ceb-44dd-be47-bf9b697296aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMachinery(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=773, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (machinery_classifier): Linear(in_features=256, out_features=62, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "machinery_model = BertForMachinery(num_machinery_labels=len(machinery_label_encoder.classes_), extra_features_dim=10) \n",
    "machinery_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b4db600-e114-4be6-834e-5d153ba444b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 및 학습률 스케줄러 설정\n",
    "optimizer_machinery = AdamW(machinery_model.parameters(), lr=2e-5)\n",
    "loss_fn_machinery=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "772d3add-051f-4e5c-b5d0-c9c7ce008715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_machinery(model, dataloader, optimizer, device, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids, attention_mask, extra_features, labels = [b.to(device) for b in batch]  # 순서 수정\n",
    "        \n",
    "        if labels.dim() > 1:\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, extra_features=extra_features)\n",
    "        labels = labels.to(torch.int64)  # CrossEntropyLoss에 맞게 변환\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9990e338-c952-4a12-ba6e-b5d960e884ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 평가 함수 - logits-62개짜리 각각의 자신감\n",
    "def evaluate_machinery(model, dataloader, device, loss_fn_machinery):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    machinery_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            input_ids, attention_mask, extra_features, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            # 모델 예측\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, extra_features=extra_features)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = loss_fn_machinery(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)  # softmax 없이 직접 logits에서 최대값 클래스 예측\n",
    "            \n",
    "            # 예측값을 저장\n",
    "            machinery_predictions.append(predicted.cpu().numpy())  # 리스트에 추가\n",
    "            \n",
    "            # 정확도 계산\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    machinery_predictions = np.concatenate(machinery_predictions, axis=0)  \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, accuracy, machinery_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a1505a3-0786-4beb-82b8-b897c930f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "381695c8-99af-44f7-b5d6-425661364fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1254 [00:00<?, ?it/s]C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:54<00:00,  5.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 2.0171, Val Loss: 1.3681, Val Acc: 0.6520, Test Acc: 0.6524\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:53<00:00,  5.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 1.2391, Val Loss: 1.0625, Val Acc: 0.7119, Test Acc: 0.7062\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:53<00:00,  5.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.9769, Val Loss: 0.8609, Val Acc: 0.7480, Test Acc: 0.7518\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:54<00:00,  5.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.7999, Val Loss: 0.7434, Val Acc: 0.7847, Test Acc: 0.7782\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:55<00:00,  5.33it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.6755, Val Loss: 0.6922, Val Acc: 0.7955, Test Acc: 0.7969\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:56<00:00,  5.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.5884, Val Loss: 0.6362, Val Acc: 0.8062, Test Acc: 0.8147\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:58<00:00,  5.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.5200, Val Loss: 0.5965, Val Acc: 0.8096, Test Acc: 0.8147\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:54<00:00,  5.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.4672, Val Loss: 0.5604, Val Acc: 0.8282, Test Acc: 0.8248\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:52<00:00,  5.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.4282, Val Loss: 0.5542, Val Acc: 0.8333, Test Acc: 0.8353\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:52<00:00,  5.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.3956, Val Loss: 0.5274, Val Acc: 0.8367, Test Acc: 0.8277\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:52<00:00,  5.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.3701, Val Loss: 0.5367, Val Acc: 0.8390, Test Acc: 0.8368\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:52<00:00,  5.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.3470, Val Loss: 0.5883, Val Acc: 0.8294, Test Acc: 0.8257\n",
      "Trigger Times (Machinery): 1\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:52<00:00,  5.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.3322, Val Loss: 0.5384, Val Acc: 0.8384, Test Acc: 0.8397\n",
      "Trigger Times (Machinery): 2\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:52<00:00,  5.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.3113, Val Loss: 0.5324, Val Acc: 0.8424, Test Acc: 0.8435\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:52<00:00,  5.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.2920, Val Loss: 0.5431, Val Acc: 0.8486, Test Acc: 0.8368\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:51<00:00,  5.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.2928, Val Loss: 0.5636, Val Acc: 0.8395, Test Acc: 0.8344\n",
      "Trigger Times (Machinery): 1\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:51<00:00,  5.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.2737, Val Loss: 0.5512, Val Acc: 0.8418, Test Acc: 0.8435\n",
      "Trigger Times (Machinery): 2\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1254/1254 [03:51<00:00,  5.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 222/222 [00:11<00:00, 19.50it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machinery - Train Loss: 0.2721, Val Loss: 0.5699, Val Acc: 0.8435, Test Acc: 0.8358\n",
      "Trigger Times (Machinery): 3\n",
      "Early stopping for Machinery!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:10<00:00, 25.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy (Machinery): 0.8358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Machinery 모델 학습 실행\n",
    "num_epochs = 40\n",
    "best_val_acc_machinery = 0\n",
    "best_val_loss_machinery = float('inf')\n",
    "best_model = None\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Machinery 모델 학습\n",
    "    train_loss_machinery = train_machinery(\n",
    "        machinery_model, \n",
    "        train_loader_machinery, \n",
    "        optimizer_machinery, \n",
    "        device, \n",
    "        loss_fn_machinery\n",
    "    )\n",
    "    \n",
    "    # Machinery 모델 평가\n",
    "    val_loss_machinery, val_acc_machinery, val_preds_machinery = evaluate_machinery(\n",
    "        machinery_model, \n",
    "        val_loader_machinery, \n",
    "        device, \n",
    "        loss_fn_machinery\n",
    "    )\n",
    "    \n",
    "    test_loss_machinery, test_acc_machinery, test_preds_machinery = evaluate_machinery(\n",
    "        machinery_model, \n",
    "        test_loader_machinery, \n",
    "        device, \n",
    "        loss_fn_machinery\n",
    "    )\n",
    "        \n",
    "    print(f\"Machinery - Train Loss: {train_loss_machinery:.4f}, Val Loss: {val_loss_machinery:.4f}, Val Acc: {val_acc_machinery:.4f}, Test Acc: {test_acc_machinery:.4f}\")\n",
    "    \n",
    "    # 현재 모델이 가장 높은 검증 정확도를 기록한 경우\n",
    "    if val_acc_machinery > best_val_acc_machinery or (val_acc_machinery == best_val_acc_machinery and val_loss_machinery < best_val_loss_machinery):\n",
    "        best_val_acc_machinery = val_acc_machinery\n",
    "        best_val_loss_machinery = val_loss_machinery\n",
    "        best_model = machinery_model  # 가장 좋은 모델 저장\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        print(f\"Trigger Times (Machinery): {trigger_times}\")\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping for Machinery!\")\n",
    "            break\n",
    "\n",
    "# 최종 최고의 모델을 피클로 저장\n",
    "with open(\"0924_best_machinery_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# 최종 테스트 성능 평가\n",
    "final_test_loss_machinery, final_test_acc_machinery, final_machinery_predictions = evaluate_machinery(\n",
    "    best_model, \n",
    "    test_loader_machinery, \n",
    "    device, \n",
    "    loss_fn_machinery\n",
    ")\n",
    "print(f\"Final Test Accuracy (Machinery): {final_test_acc_machinery:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aea936-31d1-4027-8cad-f4c5baa664b7",
   "metadata": {},
   "source": [
    "### BERT 기반 machinery 모델 학습 및 예측값\n",
    "\n",
    "> xgboost assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75369b3f-da0f-4186-9d44-e282a93035da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\ship\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMachinery(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=773, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (machinery_classifier): Linear(in_features=256, out_features=62, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# 저장된 최적의 모델을 불러옵니다 (에포크 14에서 저장된 모델)\n",
    "with open(\"0924_best_machinery_model.pkl\", 'rb') as f:\n",
    "    best_machinery_model = pickle.load(f)\n",
    "\n",
    "best_machinery_model.to(device)  # 모델을 GPU로 이동\n",
    "\n",
    "# Machinery 모델을 평가 모드로 설정\n",
    "best_machinery_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3fa58e3-c59b-4f5a-a8f3-4d0fefab3977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_seq_train_final shape: (10029, 50)\n",
      "X_seq_val shape: (1770, 50)\n",
      "X_seq_test shape: (2083, 50)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1. 어셈블리 토크나이저 피팅\n",
    "assembly_tokenizer = Tokenizer(num_words=20000)\n",
    "assembly_tokenizer.fit_on_texts(data['combined_text'])  # 필요한 데이터를 사용\n",
    "\n",
    "# 2. 텍스트를 정수 시퀀스로 변환\n",
    "sequences = assembly_tokenizer.texts_to_sequences(data['combined_text'])\n",
    "\n",
    "# 3. 시퀀스 패딩\n",
    "max_len = 50\n",
    "X_seq = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# 어셈블리 토크나이저를 파일로 저장\n",
    "with open('models/assembly_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(assembly_tokenizer, f)\n",
    "    \n",
    "# 3. 정수형 레이블 (y)\n",
    "assembly_labels = data['Assembly'].values\n",
    "\n",
    "label_encoder_assembly = LabelEncoder()\n",
    "y_assembly = label_encoder_assembly.fit_transform(assembly_labels)\n",
    "\n",
    "# Train-Test Split을 일관되게 적용\n",
    "X_seq_train, X_seq_test, y_train_assembly, y_test_assembly = train_test_split(\n",
    "    X_seq, y_assembly, test_size=0.15, random_state=42, stratify=y_assembly\n",
    ")\n",
    "\n",
    "X_seq_train_final, X_seq_val, y_train_assembly_final, y_val_assembly = train_test_split(\n",
    "    X_seq_train, y_train_assembly, test_size=0.15, random_state=42, stratify=y_train_assembly\n",
    ")\n",
    "\n",
    "# 분할된 데이터 크기 확인\n",
    "print(f\"X_seq_train_final shape: {X_seq_train_final.shape}\")\n",
    "print(f\"X_seq_val shape: {X_seq_val.shape}\")\n",
    "print(f\"X_seq_test shape: {X_seq_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b45241bc-d3fe-4051-955f-6d60e6b79432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_machinery_classes(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, extra_features = [b.to(device) for b in batch[:3]]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, extra_features=extra_features)\n",
    "            preds = torch.argmax(outputs, dim=1)  # Get the class with the highest probability\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    return np.concatenate(all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5999115a-fd7b-448c-939f-a12ebec64c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "machinery_preds_train = predict_machinery_classes(best_machinery_model, train_loader_machinery, device)\n",
    "machinery_preds_val = predict_machinery_classes(best_machinery_model, val_loader_machinery, device)\n",
    "machinery_preds_test = predict_machinery_classes(best_machinery_model, test_loader_machinery, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb06ab70-d7f2-4c73-b754-7f94f34ddb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_seq_train_with_machinery shape: (10029, 51)\n",
      "X_seq_val_with_machinery shape: (1770, 51)\n",
      "X_seq_test_with_machinery shape: (2083, 51)\n"
     ]
    }
   ],
   "source": [
    "# 예측된 machinery 클래스를 시퀀스 데이터에 추가\n",
    "X_seq_train_with_machinery = np.hstack([X_seq_train_final, machinery_preds_train.reshape(-1, 1)])\n",
    "X_seq_val_with_machinery = np.hstack([X_seq_val, machinery_preds_val.reshape(-1, 1)])\n",
    "X_seq_test_with_machinery = np.hstack([X_seq_test, machinery_preds_test.reshape(-1, 1)])\n",
    "\n",
    "# 결합된 데이터 크기 확인\n",
    "print(f\"X_seq_train_with_machinery shape: {X_seq_train_with_machinery.shape}\")\n",
    "print(f\"X_seq_val_with_machinery shape: {X_seq_val_with_machinery.shape}\")\n",
    "print(f\"X_seq_test_with_machinery shape: {X_seq_test_with_machinery.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3977014b-6c59-4eca-9d82-093e946f923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled X shape: (82185, 51)\n",
      "Resampled y shape: (82185,)\n"
     ]
    }
   ],
   "source": [
    "# 크기가 일치하는지 확인\n",
    "assert X_seq_train_with_machinery.shape[0] == y_train_assembly_final.shape[0], \"X_train과 y_train의 크기가 일치하지 않습니다.\"\n",
    "\n",
    "# SMOTE + Tomek Links 적용\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_resampled_train, y_resampled_train_assembly = smote_tomek.fit_resample(X_seq_train_with_machinery, y_train_assembly_final)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"Resampled X shape: {X_resampled_train.shape}\")\n",
    "print(f\"Resampled y shape: {y_resampled_train_assembly.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5f35cd4-c164-4bd0-ba36-96a75a5f9dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly Validation Accuracy: 0.7599\n",
      "Assembly Test Accuracy: 0.7710\n"
     ]
    }
   ],
   "source": [
    "# 5. XGBoost Assembly 모델 설정 및 학습\n",
    "assembly_model = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=209,  # Assembly 클래스 수\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1,\n",
    "    random_state=42,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "# SMOTE + Tomek Links로 불균형 데이터를 해결한 후 훈련 세트로 학습\n",
    "assembly_model.fit(X_resampled_train, y_resampled_train_assembly)\n",
    "\n",
    "# 6. 성능 평가 (검증 세트)\n",
    "assembly_preds_val = assembly_model.predict(X_seq_val_with_machinery)\n",
    "assembly_accuracy_val = accuracy_score(y_val_assembly, assembly_preds_val)\n",
    "print(f'Assembly Validation Accuracy: {assembly_accuracy_val:.4f}')\n",
    "\n",
    "# 성능 평가 (테스트 세트)\n",
    "assembly_preds_test = assembly_model.predict(X_seq_test_with_machinery)\n",
    "assembly_accuracy_test = accuracy_score(y_test_assembly, assembly_preds_test)\n",
    "print(f'Assembly Test Accuracy: {assembly_accuracy_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5e88bfa-9c08-42d5-8032-5021629c21d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best parameters for Assembly: {'colsample_bytree': 0.7949321047928289, 'gamma': 0.024029462098516863, 'learning_rate': 0.10491457315913859, 'max_depth': 8, 'min_child_weight': 2, 'n_estimators': 287, 'reg_alpha': 0.5, 'reg_lambda': 2, 'subsample': 0.8012545034320351}\n",
      "Assembly Validation Accuracy: 0.7791\n",
      "Assembly Test Accuracy: 0.7792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "import numpy as np\n",
    "\n",
    "assembly_model = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=209,  # Assembly 클래스 수에 맞게 설정\n",
    "    random_state=42,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': uniform(0.01, 0.1),  # 0.05를 중심으로 ±0.05 범위\n",
    "    'max_depth': randint(5, 12),  # 8을 중심으로 ±3\n",
    "    'n_estimators': randint(150, 300),  # 200을 중심으로 ±50\n",
    "    'subsample': uniform(0.6, 0.4),  # 0.8을 중심으로 ±0.2\n",
    "    'colsample_bytree': uniform(0.6, 0.3),  # 0.8을 중심으로 ±0.2\n",
    "    'reg_lambda': [0.5, 1, 2, 5],  # 다양한 lambda 값\n",
    "    'reg_alpha': [0, 0.5, 1, 2],  # L1 정규화 추가\n",
    "    'min_child_weight': randint(1, 6),  # 최소 자식 가중치\n",
    "    'gamma': uniform(0, 0.5),  # 트리 분할 시 요구되는 최소 손실 감소\n",
    "}\n",
    "\n",
    "random_search_assembly = RandomizedSearchCV(\n",
    "    estimator=assembly_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,  # 시도할 파라미터 조합 수를 늘릴 수 있습니다\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. RandomizedSearchCV 실행 (훈련 세트 사용)\n",
    "random_search_assembly.fit(X_seq_train_with_machinery, y_train_assembly_final)\n",
    "\n",
    "# 5. 최적의 하이퍼파라미터 출력\n",
    "print(f\"Best parameters for Assembly: {random_search_assembly.best_params_}\")\n",
    "\n",
    "# 6. 검증 세트 성능 평가\n",
    "assembly_preds_val = random_search_assembly.best_estimator_.predict(X_seq_val_with_machinery)\n",
    "assembly_accuracy_val = accuracy_score(y_val_assembly, assembly_preds_val)\n",
    "print(f'Assembly Validation Accuracy: {assembly_accuracy_val:.4f}')\n",
    "\n",
    "# 7. 테스트 세트 성능 평가\n",
    "assembly_preds_test = random_search_assembly.best_estimator_.predict(X_seq_test_with_machinery)\n",
    "assembly_accuracy_test = accuracy_score(y_test_assembly, assembly_preds_test)\n",
    "print(f'Assembly Test Accuracy: {assembly_accuracy_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "776d47d1-b055-4fcf-bf82-86e0663e646c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 0924_final_assembly_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 최적의 모델을 파일로 저장\n",
    "joblib.dump(random_search_assembly.best_estimator_, '0924_final_assembly_model.pkl')\n",
    "print(\"Model saved as 0924_final_assembly_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9779f-0b11-42d5-bb59-b8ea4afdad4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ship)",
   "language": "python",
   "name": "ship"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
